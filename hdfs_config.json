{
  "roleTypeConfigs" : [ {
    "roleType" : "BALANCER",
    "items" : [ {
      "name" : "balancer_java_heapsize",
      "value" : "1060110336",
      "required" : false,
      "default" : "1073741824",
      "displayName" : "Java Heap Size of Balancer in Bytes",
      "description" : "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rebalancing_policy",
      "required" : false,
      "default" : "DataNode",
      "displayName" : "Rebalancing Policy",
      "description" : "The policy that should be used to rebalance HDFS storage. The default DataNode policy balances the storage at the DataNode level. This is similar to the balancing policy from prior releases. The BlockPool policy balances the storage at the block pool level as well as at the DataNode level. The BlockPool policy is relevant only to a Federated HDFS service.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "balancer_java_opts",
      "required" : false,
      "default" : "",
      "displayName" : "Java Configuration Options for Balancer",
      "description" : "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rebalancer_threshold",
      "required" : false,
      "default" : "10.0",
      "displayName" : "Rebalancing Threshold",
      "description" : "The percentage deviation from average utilization, after which a node will be rebalanced. (for example, '10.0' for 10%)",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "balancer_config_safety_valve",
      "required" : false,
      "displayName" : "Balancer Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into <strong>hdfs-site.xml</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_event_whitelist",
      "required" : false,
      "default" : "{\n  \"version\": \"0\",\n  \"rules\": [\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"FATAL\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Instead, use .*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Use .* instead\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.IOException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketClosedException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.EOFException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.nio.channels.CancelledKeyException\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 2, \"exceptiontype\": \".*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Unknown job [^ ]+ being deleted.*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Error executing shell command .+ No such process.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\".*attempt to override final parameter.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"[^ ]+ is a deprecated filesystem name. Use.*\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"WARN\"}\n  ]\n}\n",
      "displayName" : "Rules to Extract Events from Log Files",
      "description" : "<p>This file contains the rules which govern how log messages are turned into events by the custom log4j appender that this role loads. It is in JSON format, and is composed of a list of rules. Every log message is evaluated against each of these rules in turn to decide whether or not to send an event for that message.</p><p>Each rule has some or all of the following fields:</p><ul><li><span class='code'>alert</span> - whether or not events generated from this rule should be promoted to alerts. A value of \"true\" will cause alerts to be generated. If not specified, the default is \"false\".</li><li><span class='code'>rate</span> <strong>(mandatory)</strong> - the maximum number of log messages matching this rule that may be sent as events every minute. If more than <tt>rate</tt> matching log messages are received in a single minute, the extra messages are ignored. If rate is less than 0, the number of messages per minute is unlimited.</li><li><span class='code'>periodminutes</span>  - the number of minutes during which the publisher will only publish <tt>rate</tt> events or fewer. If not specified, the default is <strong>one minute</strong></li><li><span class='code'>threshold</span> - apply this rule only to messages with this log4j severity level or above. An example is \"WARN\" for warning level messages or higher.</li><li><span class='code'>content</span> - match only those messages whose contents match this regular expression.</li><li><span class='code'>exceptiontype</span> - match only those messages which are part of an exception message. The exception type must match this regular expression.</li></ul><br/><p>Example:<span class='code'>{\"alert\": false, \"rate\": 10, \"exceptiontype\": \"java.lang.StringIndexOutOfBoundsException\"}</span></p><p>This rule will send events to Cloudera Manager for every <span class='code'>StringIndexOutOfBoundsException</span>, up to a maximum of 10 every minute.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    } ]
  }, {
    "roleType" : "DATANODE",
    "items" : [ {
      "name" : "dfs_data_dir_list",
      "value" : "/data/dfs/dn,/data1/dfs/dn",
      "required" : true,
      "displayName" : "DataNode Data Directory",
      "description" : "Comma-delimited list of directories on the local file system where the DataNode stores HDFS block data. Typical values are /data/N/dfs/dn for N = 1, 2, 3... These directories should be mounted using the noatime option and the disks should be configured using JBOD. RAID is not recommended. <strong>Warning: be very careful when modifying this property. Removing or changing entries can result in data loss.</strong> If you want to hot swap drives in CDH 5.4+, override the value of this property for the specific DataNode role instance whose drive is to be hot-swapped; do not modify the property value in the role group. See <link><a class=\"bold\" href=\"http://tiny.cloudera.com/hot-swap\" target=\"_blank\">Configuring Hot Swap for DataNodes<i class=\"externalLink\"></i></a></link> for more important information and caveats.",
      "relatedName" : "dfs.datanode.data.dir",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_data_dir_perm",
      "value" : "755",
      "required" : false,
      "default" : "700",
      "displayName" : "DataNode Data Directory Permissions",
      "description" : "Permissions for the directories on the local file system where the DataNode stores its blocks. The permissions must be octal. 755 and 700 are typical values.",
      "relatedName" : "dfs.datanode.data.dir.perm",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_du_reserved",
      "value" : "4023727308",
      "required" : false,
      "default" : "10737418240",
      "displayName" : "Reserved Space for Non DFS Use",
      "description" : "Reserved space in bytes per volume for non Distributed File System (DFS) use.",
      "relatedName" : "dfs.datanode.du.reserved",
      "validationState" : "OK",
      "validationMessage" : "DataNode Reserved Space for Non-DFS Use (4023727308B) is not zero."
    }, {
      "name" : "dfs_datanode_failed_volumes_tolerated",
      "value" : "1",
      "required" : false,
      "default" : "0",
      "displayName" : "DataNode Failed Volumes Tolerated",
      "description" : "The number of volumes that are allowed to fail before a DataNode stops offering service. By default, any volume failure will cause a DataNode to shutdown.",
      "relatedName" : "dfs.datanode.failed.volumes.tolerated",
      "validationState" : "OK",
      "validationMessage" : "DataNode tolerance for failed volumes (1) is less than or equal to half of the number of data directories (2)."
    }, {
      "name" : "dfs_datanode_max_locked_memory",
      "required" : false,
      "default" : "4294967296",
      "displayName" : "Maximum Memory Used for Caching",
      "description" : "The maximum amount of memory a DataNode may use to cache data blocks in memory.  Setting it to zero will disable caching.",
      "relatedName" : "dfs.datanode.max.locked.memory",
      "validationState" : "OK"
    }, {
      "name" : "dfs_thrift_threads_max",
      "required" : false,
      "default" : "20",
      "displayName" : "Hue Thrift Server Max Threadcount",
      "description" : "Maximum number of running threads for the Hue Thrift server running on each DataNode",
      "relatedName" : "dfs.thrift.threads.max",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_http_port",
      "required" : false,
      "default" : "50075",
      "displayName" : "DataNode HTTP Web UI Port",
      "description" : "Port for the DataNode HTTP web UI. Combined with the DataNode's hostname to build its HTTP address.",
      "relatedName" : "dfs.datanode.http.address",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_use_datanode_hostname",
      "required" : false,
      "default" : "false",
      "displayName" : "Use DataNode Hostname",
      "description" : "Whether DataNodes should use DataNode hostnames when connecting to DataNodes for data transfer. This property is supported in CDH3u4 or later deployments.",
      "relatedName" : "dfs.datanode.use.datanode.hostname",
      "validationState" : "OK"
    }, {
      "name" : "log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "DataNode Logging Threshold",
      "description" : "The minimum log level for DataNode logs",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_data_retention",
      "required" : false,
      "default" : "104857600",
      "displayName" : "Stacks Collection Data Retention",
      "description" : "The amount of stacks data that is retained. After the retention limit is reached, the oldest data is deleted.",
      "relatedName" : "stacks_collection_data_retention",
      "validationState" : "OK"
    }, {
      "name" : "datanode_fd_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"70.0\",\"warning\":\"50.0\"}",
      "displayName" : "File Descriptor Monitoring Thresholds",
      "description" : "The health test thresholds of the number of file descriptors used. Specified as a percentage of file descriptor limit.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_connectivity_tolerance",
      "required" : false,
      "default" : "180",
      "displayName" : "DataNode Connectivity Tolerance at Startup",
      "description" : "The amount of time to wait for the DataNode to fully start up and connect to the NameNode before enforcing the connectivity check.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_config_safety_valve",
      "required" : false,
      "displayName" : "DataNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into <strong>hdfs-site.xml</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Stacks Collection Enabled",
      "description" : "Whether or not periodic stacks collection is enabled.",
      "relatedName" : "stacks_collection_enabled",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_handler_count",
      "required" : false,
      "default" : "3",
      "displayName" : "Handler Count",
      "description" : "The number of server threads for the DataNode.",
      "relatedName" : "dfs.datanode.handler.count",
      "validationState" : "OK"
    }, {
      "name" : "enable_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Health Alerts for this Role",
      "description" : "When set, Cloudera Manager will send alerts when the health of this role reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_web_metric_collection_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"10000.0\"}",
      "displayName" : "Web Metric Collection Duration",
      "description" : "The health test thresholds on the duration of the metrics request to the web server.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_soft_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Soft Limit",
      "description" : "Soft memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process if and only if the host is facing memory pressure. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.soft_limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "process_auto_restart",
      "required" : false,
      "default" : "true",
      "displayName" : "Automatically Restart Process",
      "description" : "When set, this role's process is automatically (and transparently) restarted in the event of an unexpected failure.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_event_whitelist",
      "required" : false,
      "default" : "{\n  \"version\": \"0\",\n  \"rules\": [\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"FATAL\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Instead, use .*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Use .* instead\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.IOException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketClosedException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.EOFException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.nio.channels.CancelledKeyException\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 5, \"content\":\"Datanode registration failed\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 2, \"exceptiontype\": \".*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Got a command from standby NN - ignoring command:.*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Unknown job [^ ]+ being deleted.*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Error executing shell command .+ No such process.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\".*attempt to override final parameter.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"[^ ]+ is a deprecated filesystem name. Use.*\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"WARN\"}\n  ]\n}\n",
      "displayName" : "Rules to Extract Events from Log Files",
      "description" : "<p>This file contains the rules which govern how log messages are turned into events by the custom log4j appender that this role loads. It is in JSON format, and is composed of a list of rules. Every log message is evaluated against each of these rules in turn to decide whether or not to send an event for that message.</p><p>Each rule has some or all of the following fields:</p><ul><li><span class='code'>alert</span> - whether or not events generated from this rule should be promoted to alerts. A value of \"true\" will cause alerts to be generated. If not specified, the default is \"false\".</li><li><span class='code'>rate</span> <strong>(mandatory)</strong> - the maximum number of log messages matching this rule that may be sent as events every minute. If more than <tt>rate</tt> matching log messages are received in a single minute, the extra messages are ignored. If rate is less than 0, the number of messages per minute is unlimited.</li><li><span class='code'>periodminutes</span>  - the number of minutes during which the publisher will only publish <tt>rate</tt> events or fewer. If not specified, the default is <strong>one minute</strong></li><li><span class='code'>threshold</span> - apply this rule only to messages with this log4j severity level or above. An example is \"WARN\" for warning level messages or higher.</li><li><span class='code'>content</span> - match only those messages whose contents match this regular expression.</li><li><span class='code'>exceptiontype</span> - match only those messages which are part of an exception message. The exception type must match this regular expression.</li></ul><br/><p>Example:<span class='code'>{\"alert\": false, \"rate\": 10, \"exceptiontype\": \"java.lang.StringIndexOutOfBoundsException\"}</span></p><p>This rule will send events to Cloudera Manager for every <span class='code'>StringIndexOutOfBoundsException</span>, up to a maximum of 10 every minute.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_dir",
      "required" : false,
      "default" : "/tmp",
      "displayName" : "Heap Dump Directory",
      "description" : "Path to directory where heap dumps are generated when java.lang.OutOfMemoryError error is thrown. This directory is automatically created if it does not exist. If this directory already exists, role user must have write access to this directory. If this directory is shared among multiple roles, it should have 1777 permissions. The heap dump files are created with 600 permissions and are owned by the role user. The amount of free space in this directory should be greater than the maximum Java Process heap size configured for this role.",
      "relatedName" : "oom_heap_dump_dir",
      "validationState" : "OK"
    }, {
      "name" : "dfs_thrift_threads_min",
      "required" : false,
      "default" : "10",
      "displayName" : "Hue Thrift Server Min Threadcount",
      "description" : "Minimum number of running threads for the Hue Thrift server running on each DataNode",
      "relatedName" : "dfs.thrift.threads.min",
      "validationState" : "OK"
    }, {
      "name" : "dfs_balance_bandwidthPerSec",
      "required" : false,
      "default" : "10485760",
      "displayName" : "DataNode Balancing Bandwidth",
      "description" : "Maximum amount of bandwidth that each DataNode can use for balancing. Specified in bytes per second.",
      "relatedName" : "dfs.datanode.balance.bandwidthPerSec",
      "validationState" : "OK"
    }, {
      "name" : "datanode_pause_duration_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Pause Duration Monitoring Period",
      "description" : "The period to review when computing the moving average of extra time the pause monitor spent paused.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rlimit_fds",
      "required" : false,
      "displayName" : "Maximum Process File Descriptors",
      "description" : "If configured, overrides the process soft and hard rlimits (also called ulimits) for file descriptors to the configured value.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log4j_safety_valve",
      "required" : false,
      "displayName" : "DataNode Logging Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_size",
      "required" : false,
      "default" : "200",
      "displayName" : "DataNode Max Log Size",
      "description" : "The maximum size, in megabytes, per log file for DataNode logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "Unexpected Exits Thresholds",
      "description" : "The health test thresholds for unexpected exits encountered within a recent period specified by the unexpected_exits_window configuration for the role.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_scm_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "DataNode Process Health Test",
      "description" : "Enables the health test that the DataNode's process state is consistent with the role configuration",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_available_space_balanced_threshold",
      "required" : false,
      "default" : "10737418240",
      "displayName" : "Available Space Policy Balanced Threshold",
      "description" : "Only used when the DataNode Volume Choosing Policy is set to Available Space. Controls how much DataNode volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced. If the free space of all the volumes are within this range of each other, the volumes will be considered balanced and block assignments will be done on a pure round robin basis.",
      "relatedName" : "dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_frequency",
      "required" : false,
      "default" : "5.0",
      "displayName" : "Stacks Collection Frequency",
      "description" : "The frequency with which stacks are collected.",
      "relatedName" : "stacks_collection_frequency",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Log Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Log Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_max_xcievers",
      "required" : false,
      "default" : "4096",
      "displayName" : "Maximum Number of Transfer Threads",
      "description" : "Specifies the maximum number of threads to use for transferring data in and out of the DataNode.",
      "relatedName" : "dfs.datanode.max.transfer.threads",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Log Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Dump Heap When Out of Memory",
      "description" : "When set, generates heap dump file when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_backup_index",
      "required" : false,
      "default" : "10",
      "displayName" : "DataNode Maximum Log File Backups",
      "description" : "The maximum number of rolled log files to keep for DataNode logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_transceivers_usage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"95.0\",\"warning\":\"75.0\"}",
      "displayName" : "DataNode Transceivers Usage Thresholds",
      "description" : "The health test thresholds of transceivers usage in a DataNode. Specified as a percentage of the total configured number of transceivers.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Unexpected Exits Monitoring Period",
      "description" : "The period to review when computing unexpected exits.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Heap Dump Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_java_opts",
      "required" : false,
      "default" : "-XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled",
      "displayName" : "Java Configuration Options for DataNode",
      "description" : "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_method",
      "required" : false,
      "default" : "jstack",
      "displayName" : "Stacks Collection Method",
      "description" : "The method used to collect stacks. The jstack option involves periodically running the jstack command against the role's daemon process. The servlet method is available for those roles that have an HTTP server endpoint exposing the current stacks traces of all threads. When the servlet method is selected, that HTTP endpoint is periodically scraped.",
      "relatedName" : "stacks_collection_method",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_plugins_list",
      "required" : false,
      "default" : "",
      "displayName" : "DateNode Plugins",
      "description" : "Comma-separated list of DataNode plug-ins to be activated. If one plug-in cannot be loaded, all the plug-ins are ignored.",
      "relatedName" : "dfs.datanode.plugins",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_ipc_port",
      "required" : false,
      "default" : "50020",
      "displayName" : "DataNode Protocol Port",
      "description" : "Port for the various DataNode Protocols. Combined with the DataNode's hostname to build its IPC port address.",
      "relatedName" : "dfs.datanode.ipc.address",
      "validationState" : "OK"
    }, {
      "name" : "datanode_host_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "DataNode Host Health Test",
      "description" : "When computing the overall DataNode health, consider the host's health.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_bind_wildcard",
      "required" : false,
      "default" : "false",
      "displayName" : "Bind DataNode to Wildcard Address",
      "description" : "If enabled, the DataNode binds to the wildcard address (\"0.0.0.0\") on all of its ports.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_drop_cache_behind_reads",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable purging cache after reads",
      "description" : "In some workloads, the data read from HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is delivered to the client. This may improve performance for some workloads by freeing buffer cache spare usage for more cacheable data. This behavior will always be disabled for workloads that read only short sections of a block (e.g HBase random-IO workloads). This property is supported in CDH3u3 or later deployments.",
      "relatedName" : "dfs.datanode.drop.cache.behind.reads",
      "validationState" : "OK"
    }, {
      "name" : "dfs_thrift_timeout",
      "required" : false,
      "default" : "60",
      "displayName" : "Hue Thrift Server Timeout",
      "description" : "Timeout in seconds for the Hue Thrift server running on each DataNode",
      "relatedName" : "dfs.thrift.timeout",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_available_space_balanced_preference",
      "required" : false,
      "default" : "0.75",
      "displayName" : "Available Space Policy Balanced Preference",
      "description" : "Only used when the DataNode Volume Choosing Policy is set to Available Space. Controls what percentage of new block allocations will be sent to volumes with more available disk space than others. This setting should be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should be no reason to prefer that volumes with less available disk space receive more block allocations.",
      "relatedName" : "dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_readahead_bytes",
      "required" : false,
      "default" : "4194304",
      "displayName" : "Number of read ahead bytes",
      "description" : "While reading block files, the DataNode can use the posix_fadvise system call to explicitly page data into the operating system buffer cache ahead of the current reader's position. This can improve performance especially when disks are highly contended. This configuration specifies the number of bytes ahead of the current read position which the DataNode will attempt to read ahead. A value of 0 disables this feature. This property is supported in CDH3u3 or later deployments.",
      "relatedName" : "dfs.datanode.readahead.bytes",
      "validationState" : "OK"
    }, {
      "name" : "oom_sigkill_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Kill When Out of Memory",
      "description" : "When set, a SIGKILL signal is sent to the role process when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_sync_behind_writes",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable immediate enqueuing of data to disk after writes",
      "description" : "If this configuration is enabled, the DataNode will instruct the operating system to enqueue all written data to the disk immediately after it is written. This differs from the usual OS policy which may wait for up to 30 seconds before triggering writeback. This may improve performance for some workloads by smoothing the IO profile for data written to disk. This property is supported in CDH3u3 or later deployments.",
      "relatedName" : "dfs.datanode.sync.behind.writes",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_https_port",
      "required" : false,
      "default" : "50475",
      "displayName" : "Secure DataNode Web UI Port (SSL)",
      "description" : "The base port where the secure DataNode web UI listens.  Combined with the DataNode's hostname to build its secure web UI address.",
      "relatedName" : "dfs.datanode.https.address",
      "validationState" : "OK"
    }, {
      "name" : "datanode_log_dir",
      "required" : false,
      "default" : "/var/log/hadoop-hdfs",
      "displayName" : "DataNode Log Directory",
      "description" : "Directory where DataNode will place its log files.",
      "relatedName" : "hadoop.log.dir",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_volume_choosing_policy",
      "required" : false,
      "default" : "org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy",
      "displayName" : "DataNode Volume Choosing Policy",
      "description" : "DataNode Policy for picking which volume should get a new block. The Available Space policy is only available starting with CDH 4.3.",
      "relatedName" : "dfs.datanode.fsdataset.volume.choosing.policy",
      "validationState" : "OK"
    }, {
      "name" : "datanode_free_space_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"10.0\",\"warning\":\"20.0\"}",
      "displayName" : "DataNode Free Space Monitoring Thresholds",
      "description" : "The health test thresholds of free space in a DataNode. Specified as a percentage of the capacity on the DataNode.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_hard_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Hard Limit",
      "description" : "Hard memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_drop_cache_behind_writes",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable purging cache after writes",
      "description" : "In some workloads, the data written to HDFS is known to be significantly large enough that it is unlikely to be useful to cache it in the operating system buffer cache. In this case, the DataNode may be configured to automatically purge all data from the buffer cache after it is written to disk. This may improve performance for some workloads by freeing buffer cache spare usage for more cacheable data. This property is supported in CDH3u3 or later deployments.",
      "relatedName" : "dfs.datanode.drop.cache.behind.writes",
      "validationState" : "OK"
    }, {
      "name" : "rm_cpu_shares",
      "required" : false,
      "default" : "1024",
      "displayName" : "Cgroup CPU Shares",
      "description" : "Number of CPU shares to assign to this role. The greater the number of shares, the larger the share of the host's CPUs that will be given to this role when the host experiences CPU contention. Must be between 2 and 262144. Defaults to 1024 for processes not managed by Cloudera Manager.",
      "relatedName" : "cpu.shares",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_directory",
      "required" : false,
      "displayName" : "Stacks Collection Directory",
      "description" : "The directory in which stacks logs are placed. If not set, stacks are logged into a <span class='code'>stacks</span> subdirectory of the role's log directory.",
      "relatedName" : "stacks_collection_directory",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_io_weight",
      "required" : false,
      "default" : "500",
      "displayName" : "Cgroup I/O Weight",
      "description" : "Weight for the read I/O requests issued by this role. The greater the weight, the higher the priority of the requests when the host experiences I/O contention. Must be between 100 and 1000. Defaults to 1000 for processes not managed by Cloudera Manager.",
      "relatedName" : "blkio.weight",
      "validationState" : "OK"
    }, {
      "name" : "hadoop_metrics2_safety_valve",
      "required" : false,
      "displayName" : "Hadoop Metrics2 Advanced Configuration Snippet (Safety Valve)",
      "description" : "Advanced Configuration Snippet (Safety Valve) for Hadoop Metrics2. Properties will be inserted into <strong>hadoop-metrics2.properties</strong>.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_volume_failures_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "DataNode Volume Failures Thresholds",
      "description" : "The health test thresholds of failed volumes in a DataNode.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_connectivity_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "DataNode Connectivity Health Test",
      "description" : "Enables the health test that verifies the DataNode is connected to the NameNode",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "DATANODE_role_env_safety_valve",
      "required" : false,
      "displayName" : "DataNode Environment Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of this role except client configuration.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_java_heapsize",
      "required" : false,
      "default" : "1073741824",
      "displayName" : "Java Heap Size of DataNode in Bytes",
      "description" : "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_web_metric_collection_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Web Metric Collection",
      "description" : "Enables the health test that the Cloudera Manager Agent can successfully contact and gather metrics from the web server.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_pause_duration_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"60.0\",\"warning\":\"30.0\"}",
      "displayName" : "Pause Duration Thresholds",
      "description" : "The health test thresholds for the weighted average extra time the pause monitor spent paused. Specified as a percentage of elapsed wall clock time.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "role_triggers",
      "required" : false,
      "default" : "[]",
      "displayName" : "Role Triggers",
      "description" : "<p>The configured triggers for this role. This is a JSON formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has all of the following fields:</p><ul><li><code>triggerName</code> <strong>(mandatory)</strong> - The name of the trigger. This value must be unique for the specific role. </li><li><code>triggerExpression</code> <strong>(mandatory)</strong> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <strong>(optional)</strong> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <strong> (optional)</strong> - By default set to 'true'. If set to 'false', the trigger will not be evaluated.</li><li><code>expressionEditorConfig</code> <strong> (optional)</strong> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here may lead to inconsistencies.</li></ul></p><p>For example, the following JSON formatted trigger configured for a DataNode fires if the DataNode has more than 1500 file-descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleName=$ROLENAME and last(fd_open) > 1500) DO health:bad\",\n  \"streamThreshold\": 0, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change in the future and, as a result, backward compatibility is not guaranteed between releases at this time.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "process_swap_memory_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"any\"}",
      "displayName" : "Process Swap Memory Thresholds",
      "description" : "The health test thresholds on the swap memory usage of the process.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "datanode_block_count_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"500000.0\"}",
      "displayName" : "DataNode Block Count Thresholds",
      "description" : "The health test thresholds of the number of blocks on a DataNode",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_datanode_port",
      "required" : false,
      "default" : "50010",
      "displayName" : "DataNode Transceiver Port",
      "description" : "Port for DataNode's XCeiver Protocol. Combined with the DataNode's hostname to build its address.",
      "relatedName" : "dfs.datanode.address",
      "validationState" : "OK"
    } ]
  }, {
    "roleType" : "FAILOVERCONTROLLER",
    "items" : [ {
      "name" : "ha_health_monitor_rpc_timeout_ms",
      "required" : false,
      "default" : "45000",
      "displayName" : "HA Health Monitor RPC Timeout",
      "description" : "The RPC timeout for the HA health monitor.",
      "relatedName" : "ha.health-monitor.rpc-timeout.ms",
      "validationState" : "OK"
    }, {
      "name" : "log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "Failover Controller Logging Threshold",
      "description" : "The minimum log level for Failover Controller logs",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_data_retention",
      "required" : false,
      "default" : "104857600",
      "displayName" : "Stacks Collection Data Retention",
      "description" : "The amount of stacks data that is retained. After the retention limit is reached, the oldest data is deleted.",
      "relatedName" : "stacks_collection_data_retention",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Stacks Collection Enabled",
      "description" : "Whether or not periodic stacks collection is enabled.",
      "relatedName" : "stacks_collection_enabled",
      "validationState" : "OK"
    }, {
      "name" : "oom_sigkill_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Kill When Out of Memory",
      "description" : "When set, a SIGKILL signal is sent to the role process when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_alerts",
      "required" : false,
      "default" : "true",
      "displayName" : "Enable Health Alerts for this Role",
      "description" : "When set, Cloudera Manager will send alerts when the health of this role reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "fc_config_safety_valve",
      "required" : false,
      "displayName" : "Failover Controller Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into <strong>hdfs-site.xml</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_hard_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Hard Limit",
      "description" : "Hard memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "failover_controller_java_heapsize",
      "required" : false,
      "default" : "268435456",
      "displayName" : "Java Heap Size of Failover Controller in Bytes",
      "description" : "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_soft_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Soft Limit",
      "description" : "Soft memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process if and only if the host is facing memory pressure. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.soft_limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "process_auto_restart",
      "required" : false,
      "default" : "false",
      "displayName" : "Automatically Restart Process",
      "description" : "When set, this role's process is automatically (and transparently) restarted in the event of an unexpected failure.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_cpu_shares",
      "required" : false,
      "default" : "1024",
      "displayName" : "Cgroup CPU Shares",
      "description" : "Number of CPU shares to assign to this role. The greater the number of shares, the larger the share of the host's CPUs that will be given to this role when the host experiences CPU contention. Must be between 2 and 262144. Defaults to 1024 for processes not managed by Cloudera Manager.",
      "relatedName" : "cpu.shares",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_dir",
      "required" : false,
      "default" : "/tmp",
      "displayName" : "Heap Dump Directory",
      "description" : "Path to directory where heap dumps are generated when java.lang.OutOfMemoryError error is thrown. This directory is automatically created if it does not exist. If this directory already exists, role user must have write access to this directory. If this directory is shared among multiple roles, it should have 1777 permissions. The heap dump files are created with 600 permissions and are owned by the role user. The amount of free space in this directory should be greater than the maximum Java Process heap size configured for this role.",
      "relatedName" : "oom_heap_dump_dir",
      "validationState" : "OK"
    }, {
      "name" : "log_event_whitelist",
      "required" : false,
      "default" : "{\n  \"version\": \"0\",\n  \"rules\": [\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"FATAL\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 2, \"exceptiontype\": \".*\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"WARN\"}\n  ]\n}",
      "displayName" : "Rules to Extract Events from Log Files",
      "description" : "<p>This file contains the rules which govern how log messages are turned into events by the custom log4j appender that this role loads. It is in JSON format, and is composed of a list of rules. Every log message is evaluated against each of these rules in turn to decide whether or not to send an event for that message.</p><p>Each rule has some or all of the following fields:</p><ul><li><span class='code'>alert</span> - whether or not events generated from this rule should be promoted to alerts. A value of \"true\" will cause alerts to be generated. If not specified, the default is \"false\".</li><li><span class='code'>rate</span> <strong>(mandatory)</strong> - the maximum number of log messages matching this rule that may be sent as events every minute. If more than <tt>rate</tt> matching log messages are received in a single minute, the extra messages are ignored. If rate is less than 0, the number of messages per minute is unlimited.</li><li><span class='code'>periodminutes</span>  - the number of minutes during which the publisher will only publish <tt>rate</tt> events or fewer. If not specified, the default is <strong>one minute</strong></li><li><span class='code'>threshold</span> - apply this rule only to messages with this log4j severity level or above. An example is \"WARN\" for warning level messages or higher.</li><li><span class='code'>content</span> - match only those messages whose contents match this regular expression.</li><li><span class='code'>exceptiontype</span> - match only those messages which are part of an exception message. The exception type must match this regular expression.</li></ul><br/><p>Example:<span class='code'>{\"alert\": false, \"rate\": 10, \"exceptiontype\": \"java.lang.StringIndexOutOfBoundsException\"}</span></p><p>This rule will send events to Cloudera Manager for every <span class='code'>StringIndexOutOfBoundsException</span>, up to a maximum of 10 every minute.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "failovercontroller_host_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Failover Controller Host Health Test",
      "description" : "When computing the overall Failover Controller health, consider the host's health.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_directory",
      "required" : false,
      "displayName" : "Stacks Collection Directory",
      "description" : "The directory in which stacks logs are placed. If not set, stacks are logged into a <span class='code'>stacks</span> subdirectory of the role's log directory.",
      "relatedName" : "stacks_collection_directory",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "failovercontroller_scm_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Failover Controller Process Health Test",
      "description" : "Enables the health test that the Failover Controller's process state is consistent with the role configuration",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "failover_controller_java_opts",
      "required" : false,
      "default" : "",
      "displayName" : "Java Configuration Options for Failover Controller",
      "description" : "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_io_weight",
      "required" : false,
      "default" : "500",
      "displayName" : "Cgroup I/O Weight",
      "description" : "Weight for the read I/O requests issued by this role. The greater the weight, the higher the priority of the requests when the host experiences I/O contention. Must be between 100 and 1000. Defaults to 1000 for processes not managed by Cloudera Manager.",
      "relatedName" : "blkio.weight",
      "validationState" : "OK"
    }, {
      "name" : "rlimit_fds",
      "required" : false,
      "displayName" : "Maximum Process File Descriptors",
      "description" : "If configured, overrides the process soft and hard rlimits (also called ulimits) for file descriptors to the configured value.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log4j_safety_valve",
      "required" : false,
      "displayName" : "Failover Controller Logging Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_size",
      "required" : false,
      "default" : "200",
      "displayName" : "Failover Controller Max Log Size",
      "description" : "The maximum size, in megabytes, per log file for Failover Controller logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "Unexpected Exits Thresholds",
      "description" : "The health test thresholds for unexpected exits encountered within a recent period specified by the unexpected_exits_window configuration for the role.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "FAILOVERCONTROLLER_role_env_safety_valve",
      "required" : false,
      "displayName" : "Failover Controller Environment Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of this role except client configuration.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_frequency",
      "required" : false,
      "default" : "5.0",
      "displayName" : "Stacks Collection Frequency",
      "description" : "The frequency with which stacks are collected.",
      "relatedName" : "stacks_collection_frequency",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Log Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Log Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "role_triggers",
      "required" : false,
      "default" : "[]",
      "displayName" : "Role Triggers",
      "description" : "<p>The configured triggers for this role. This is a JSON formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has all of the following fields:</p><ul><li><code>triggerName</code> <strong>(mandatory)</strong> - The name of the trigger. This value must be unique for the specific role. </li><li><code>triggerExpression</code> <strong>(mandatory)</strong> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <strong>(optional)</strong> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <strong> (optional)</strong> - By default set to 'true'. If set to 'false', the trigger will not be evaluated.</li><li><code>expressionEditorConfig</code> <strong> (optional)</strong> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here may lead to inconsistencies.</li></ul></p><p>For example, the following JSON formatted trigger configured for a DataNode fires if the DataNode has more than 1500 file-descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleName=$ROLENAME and last(fd_open) > 1500) DO health:bad\",\n  \"streamThreshold\": 0, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change in the future and, as a result, backward compatibility is not guaranteed between releases at this time.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "process_swap_memory_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"any\"}",
      "displayName" : "Process Swap Memory Thresholds",
      "description" : "The health test thresholds on the swap memory usage of the process.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Log Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Dump Heap When Out of Memory",
      "description" : "When set, generates heap dump file when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_backup_index",
      "required" : false,
      "default" : "10",
      "displayName" : "Failover Controller Maximum Log File Backups",
      "description" : "The maximum number of rolled log files to keep for Failover Controller logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Unexpected Exits Monitoring Period",
      "description" : "The period to review when computing unexpected exits.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Heap Dump Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_method",
      "required" : false,
      "default" : "jstack",
      "displayName" : "Stacks Collection Method",
      "description" : "The method used to collect stacks. The jstack option involves periodically running the jstack command against the role's daemon process. The servlet method is available for those roles that have an HTTP server endpoint exposing the current stacks traces of all threads. When the servlet method is selected, that HTTP endpoint is periodically scraped.",
      "relatedName" : "stacks_collection_method",
      "validationState" : "OK"
    }, {
      "name" : "failovercontroller_fd_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"70.0\",\"warning\":\"50.0\"}",
      "displayName" : "File Descriptor Monitoring Thresholds",
      "description" : "The health test thresholds of the number of file descriptors used. Specified as a percentage of file descriptor limit.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "failover_controller_log_dir",
      "required" : false,
      "default" : "/var/log/hadoop-hdfs",
      "displayName" : "Failover Controller Log Directory",
      "description" : "Directory where Failover Controller will place its log files.",
      "relatedName" : "",
      "validationState" : "OK"
    } ]
  }, {
    "roleType" : "GATEWAY",
    "items" : [ {
      "name" : "dfs_client_use_trash",
      "value" : "true",
      "required" : false,
      "default" : "false",
      "displayName" : "Use Trash",
      "description" : "Move deleted files to the trash so that they can be recovered if necessary. This client side configuration takes effect only if the HDFS service-wide trash is disabled (NameNode Filesystem Trash Interval set to 0) and is ignored otherwise. The trash is not automatically emptied when enabled with this configuration.",
      "relatedName" : "",
      "validationState" : "OK",
      "validationMessage" : "HDFS Trash is enabled."
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "client_config_priority",
      "required" : false,
      "default" : "90",
      "displayName" : "Alternatives Priority",
      "description" : "The priority level that the client configuration will have in the Alternatives system on the hosts. Higher priority levels will cause Alternatives to prefer this configuration over any others.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "client_config_root_dir",
      "required" : false,
      "default" : "/etc/hadoop",
      "displayName" : "Deploy Directory",
      "description" : "The directory where the client configs will be deployed",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "hdfs_client_config_safety_valve",
      "required" : false,
      "displayName" : "HDFS Client Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into the client configuration for <strong>hdfs-site.xml</strong>.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "hdfs_client_java_heapsize",
      "required" : false,
      "default" : "268435456",
      "displayName" : "Client Java Heap Size in Bytes",
      "description" : "Maximum size in bytes for the Java process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "Gateway Logging Threshold",
      "description" : "The minimum log level for Gateway logs",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_client_read_shortcircuit",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable HDFS Short Circuit Read",
      "description" : "Enable HDFS short circuit read. This allows a client co-located with the DataNode to read HDFS file blocks directly. This gives a performance boost to distributed clients that are aware of locality.",
      "relatedName" : "dfs.client.read.shortcircuit",
      "validationState" : "OK"
    }, {
      "name" : "hbase_client_java_opts",
      "required" : false,
      "default" : "-Djava.net.preferIPv4Stack=true",
      "displayName" : "Client Java Configuration Options",
      "description" : "These are Java command line arguments. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log4j_safety_valve",
      "required" : false,
      "displayName" : "Gateway Logging Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "hdfs_client_env_safety_valve",
      "required" : false,
      "displayName" : "HDFS Client Environment Advanced Configuration Snippet (Safety Valve) for hadoop-env.sh",
      "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into the client configuration for <strong>hadoop-env.sh</strong>",
      "relatedName" : "",
      "validationState" : "OK"
    } ]
  }, {
    "roleType" : "HTTPFS",
    "items" : [ {
      "name" : "httpfs_java_heapsize",
      "required" : false,
      "default" : "268435456",
      "displayName" : "Java Heap Size of HttpFS in Bytes",
      "description" : "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "kerberos_role_princ_name",
      "required" : false,
      "default" : "httpfs",
      "displayName" : "Role-Specific Kerberos Principal",
      "description" : "Kerberos principal used by the HttpFS roles.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "HttpFS Logging Threshold",
      "description" : "The minimum log level for HttpFS logs",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_data_retention",
      "required" : false,
      "default" : "104857600",
      "displayName" : "Stacks Collection Data Retention",
      "description" : "The amount of stacks data that is retained. After the retention limit is reached, the oldest data is deleted.",
      "relatedName" : "stacks_collection_data_retention",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_log_dir",
      "required" : false,
      "default" : "/var/log/hadoop-httpfs",
      "displayName" : "HttpFS Log Directory",
      "description" : "Directory where HttpFS will place its log files.",
      "relatedName" : "hadoop.log.dir",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Stacks Collection Enabled",
      "description" : "Whether or not periodic stacks collection is enabled.",
      "relatedName" : "stacks_collection_enabled",
      "validationState" : "OK"
    }, {
      "name" : "oom_sigkill_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Kill When Out of Memory",
      "description" : "When set, a SIGKILL signal is sent to the role process when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_load_balancer",
      "required" : false,
      "displayName" : "HttpFS Load Balancer",
      "description" : "Address of the load balancer used for HttpFS roles. Should be specified in host:port format. <b>Note:</b> Changing this property will regenerate Kerberos keytabs for all HttpFS roles.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_https_keystore_file",
      "required" : false,
      "default" : "/var/run/hadoop-httpfs/.keystore",
      "displayName" : "HttpFS Keystore File",
      "description" : "Location of the keystore file used by HttpFS role for SSL.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_alerts",
      "required" : false,
      "default" : "true",
      "displayName" : "Enable Health Alerts for this Role",
      "description" : "When set, Cloudera Manager will send alerts when the health of this role reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_hard_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Hard Limit",
      "description" : "Hard memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_https_truststore_file",
      "required" : false,
      "displayName" : "HttpFS TLS/SSL Certificate Trust Store File",
      "description" : "The location on disk of the trust store, in .jks format, used to confirm the authenticity of TLS/SSL servers that HttpFS might connect to. This is used when HttpFS is the client in a TLS/SSL connection. This trust store must contain the certificate(s) used to sign the service(s) being connected to. If this parameter is not provided, the default list of well-known certificate authorities is used instead.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_soft_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Soft Limit",
      "description" : "Soft memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process if and only if the host is facing memory pressure. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.soft_limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_java_opts",
      "required" : false,
      "default" : "",
      "displayName" : "Java Configuration Options for HttpFS",
      "description" : "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_fd_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"70.0\",\"warning\":\"50.0\"}",
      "displayName" : "File Descriptor Monitoring Thresholds",
      "description" : "The health test thresholds of the number of file descriptors used. Specified as a percentage of file descriptor limit.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_https_truststore_password",
      "required" : false,
      "displayName" : "HttpFS TLS/SSL Certificate Trust Store Password",
      "description" : "The password for the HttpFS TLS/SSL Certificate Trust Store File. Note that this password is not required to access the trust store: this field can be left blank. This password provides optional integrity checking of the file. The contents of trust stores are certificates, and certificates are public information.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_cpu_shares",
      "required" : false,
      "default" : "1024",
      "displayName" : "Cgroup CPU Shares",
      "description" : "Number of CPU shares to assign to this role. The greater the number of shares, the larger the share of the host's CPUs that will be given to this role when the host experiences CPU contention. Must be between 2 and 262144. Defaults to 1024 for processes not managed by Cloudera Manager.",
      "relatedName" : "cpu.shares",
      "validationState" : "OK"
    }, {
      "name" : "process_auto_restart",
      "required" : false,
      "default" : "false",
      "displayName" : "Automatically Restart Process",
      "description" : "When set, this role's process is automatically (and transparently) restarted in the event of an unexpected failure.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_dir",
      "required" : false,
      "default" : "/tmp",
      "displayName" : "Heap Dump Directory",
      "description" : "Path to directory where heap dumps are generated when java.lang.OutOfMemoryError error is thrown. This directory is automatically created if it does not exist. If this directory already exists, role user must have write access to this directory. If this directory is shared among multiple roles, it should have 1777 permissions. The heap dump files are created with 600 permissions and are owned by the role user. The amount of free space in this directory should be greater than the maximum Java Process heap size configured for this role.",
      "relatedName" : "oom_heap_dump_dir",
      "validationState" : "OK"
    }, {
      "name" : "hdfs_httpfs_signature_secret",
      "required" : false,
      "default" : "hadoop httpfs secret",
      "displayName" : "Signature Secret",
      "description" : "The secret to use for signing client authentication tokens.",
      "relatedName" : "hdfs.httpfs.signature.secret",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_directory",
      "required" : false,
      "displayName" : "Stacks Collection Directory",
      "description" : "The directory in which stacks logs are placed. If not set, stacks are logged into a <span class='code'>stacks</span> subdirectory of the role's log directory.",
      "relatedName" : "stacks_collection_directory",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_scm_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "HttpFS Process Health Test",
      "description" : "Enables the health test that the HttpFS's process state is consistent with the role configuration",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_io_weight",
      "required" : false,
      "default" : "500",
      "displayName" : "Cgroup I/O Weight",
      "description" : "Weight for the read I/O requests issued by this role. The greater the weight, the higher the priority of the requests when the host experiences I/O contention. Must be between 100 and 1000. Defaults to 1000 for processes not managed by Cloudera Manager.",
      "relatedName" : "blkio.weight",
      "validationState" : "OK"
    }, {
      "name" : "hdfs_httpfs_admin_port",
      "required" : false,
      "default" : "14001",
      "displayName" : "Administration Port",
      "description" : "The port for the administration interface.",
      "relatedName" : "hdfs.httpfs.admin.port",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_https_keystore_password",
      "required" : false,
      "displayName" : "HttpFS Keystore Password",
      "description" : "Password of the keystore used by HttpFS role for SSL.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rlimit_fds",
      "required" : false,
      "displayName" : "Maximum Process File Descriptors",
      "description" : "If configured, overrides the process soft and hard rlimits (also called ulimits) for file descriptors to the configured value.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log4j_safety_valve",
      "required" : false,
      "displayName" : "HttpFS Logging Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_size",
      "required" : false,
      "default" : "200",
      "displayName" : "HttpFS Max Log Size",
      "description" : "The maximum size, in megabytes, per log file for HttpFS logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "Unexpected Exits Thresholds",
      "description" : "The health test thresholds for unexpected exits encountered within a recent period specified by the unexpected_exits_window configuration for the role.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "hdfs_httpfs_http_port",
      "required" : false,
      "default" : "14000",
      "displayName" : "REST Port",
      "description" : "The port where the REST interface to HDFS is available. The REST interface is served over HTTPS if SSL is enabled for HttpFS, or over HTTP otherwise.",
      "relatedName" : "hdfs.httpfs.http.port",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_config_safety_valve",
      "required" : false,
      "displayName" : "HttpFS Advanced Configuration Snippet (Safety Valve) for httpfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into <strong>httpfs-site.xml</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_frequency",
      "required" : false,
      "default" : "5.0",
      "displayName" : "Stacks Collection Frequency",
      "description" : "The frequency with which stacks are collected.",
      "relatedName" : "stacks_collection_frequency",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Log Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Log Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_process_groupname",
      "required" : false,
      "default" : "httpfs",
      "displayName" : "System Group",
      "description" : "The group that the HttpFS server process should run as.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "HTTPFS_role_env_safety_valve",
      "required" : false,
      "displayName" : "HttpFS Environment Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of this role except client configuration.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "role_triggers",
      "required" : false,
      "default" : "[]",
      "displayName" : "Role Triggers",
      "description" : "<p>The configured triggers for this role. This is a JSON formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has all of the following fields:</p><ul><li><code>triggerName</code> <strong>(mandatory)</strong> - The name of the trigger. This value must be unique for the specific role. </li><li><code>triggerExpression</code> <strong>(mandatory)</strong> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <strong>(optional)</strong> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <strong> (optional)</strong> - By default set to 'true'. If set to 'false', the trigger will not be evaluated.</li><li><code>expressionEditorConfig</code> <strong> (optional)</strong> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here may lead to inconsistencies.</li></ul></p><p>For example, the following JSON formatted trigger configured for a DataNode fires if the DataNode has more than 1500 file-descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleName=$ROLENAME and last(fd_open) > 1500) DO health:bad\",\n  \"streamThreshold\": 0, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change in the future and, as a result, backward compatibility is not guaranteed between releases at this time.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "process_swap_memory_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"any\"}",
      "displayName" : "Process Swap Memory Thresholds",
      "description" : "The health test thresholds on the swap memory usage of the process.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Log Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Dump Heap When Out of Memory",
      "description" : "When set, generates heap dump file when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_process_username",
      "required" : false,
      "default" : "httpfs",
      "displayName" : "System User",
      "description" : "The user that the HttpFS server process should run as.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_backup_index",
      "required" : false,
      "default" : "10",
      "displayName" : "HttpFS Maximum Log File Backups",
      "description" : "The maximum number of rolled log files to keep for HttpFS logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_host_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "HttpFS Host Health Test",
      "description" : "When computing the overall HttpFS health, consider the host's health.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Unexpected Exits Monitoring Period",
      "description" : "The period to review when computing unexpected exits.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Heap Dump Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_method",
      "required" : false,
      "default" : "jstack",
      "displayName" : "Stacks Collection Method",
      "description" : "The method used to collect stacks. The jstack option involves periodically running the jstack command against the role's daemon process. The servlet method is available for those roles that have an HTTP server endpoint exposing the current stacks traces of all threads. When the servlet method is selected, that HTTP endpoint is periodically scraped.",
      "relatedName" : "stacks_collection_method",
      "validationState" : "OK"
    }, {
      "name" : "httpfs_use_ssl",
      "required" : false,
      "default" : "false",
      "displayName" : "Use SSL",
      "description" : "Use SSL for HttpFS.",
      "relatedName" : "",
      "validationState" : "OK"
    } ]
  }, {
    "roleType" : "JOURNALNODE",
    "items" : [ {
      "name" : "dfs_journalnode_edits_dir",
      "required" : true,
      "displayName" : "JournalNode Edits Directory",
      "description" : "Directory on the local file system where NameNode edits are written.",
      "relatedName" : "dfs.journalnode.edits.dir",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_data_retention",
      "required" : false,
      "default" : "104857600",
      "displayName" : "Stacks Collection Data Retention",
      "description" : "The amount of stacks data that is retained. After the retention limit is reached, the oldest data is deleted.",
      "relatedName" : "stacks_collection_data_retention",
      "validationState" : "OK"
    }, {
      "name" : "log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "JournalNode Logging Threshold",
      "description" : "The minimum log level for JournalNode logs",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Stacks Collection Enabled",
      "description" : "Whether or not periodic stacks collection is enabled.",
      "relatedName" : "stacks_collection_enabled",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_fd_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"70.0\",\"warning\":\"50.0\"}",
      "displayName" : "File Descriptor Monitoring Thresholds",
      "description" : "The health test thresholds of the number of file descriptors used. Specified as a percentage of file descriptor limit.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_alerts",
      "required" : false,
      "default" : "true",
      "displayName" : "Enable Health Alerts for this Role",
      "description" : "When set, Cloudera Manager will send alerts when the health of this role reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_soft_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Soft Limit",
      "description" : "Soft memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process if and only if the host is facing memory pressure. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.soft_limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "process_auto_restart",
      "required" : false,
      "default" : "true",
      "displayName" : "Automatically Restart Process",
      "description" : "When set, this role's process is automatically (and transparently) restarted in the event of an unexpected failure.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_dir",
      "required" : false,
      "default" : "/tmp",
      "displayName" : "Heap Dump Directory",
      "description" : "Path to directory where heap dumps are generated when java.lang.OutOfMemoryError error is thrown. This directory is automatically created if it does not exist. If this directory already exists, role user must have write access to this directory. If this directory is shared among multiple roles, it should have 1777 permissions. The heap dump files are created with 600 permissions and are owned by the role user. The amount of free space in this directory should be greater than the maximum Java Process heap size configured for this role.",
      "relatedName" : "oom_heap_dump_dir",
      "validationState" : "OK"
    }, {
      "name" : "log_event_whitelist",
      "required" : false,
      "default" : "{\n  \"version\": \"0\",\n  \"rules\": [\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"FATAL\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 2, \"exceptiontype\": \".*\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"WARN\"}\n  ]\n}",
      "displayName" : "Rules to Extract Events from Log Files",
      "description" : "<p>This file contains the rules which govern how log messages are turned into events by the custom log4j appender that this role loads. It is in JSON format, and is composed of a list of rules. Every log message is evaluated against each of these rules in turn to decide whether or not to send an event for that message.</p><p>Each rule has some or all of the following fields:</p><ul><li><span class='code'>alert</span> - whether or not events generated from this rule should be promoted to alerts. A value of \"true\" will cause alerts to be generated. If not specified, the default is \"false\".</li><li><span class='code'>rate</span> <strong>(mandatory)</strong> - the maximum number of log messages matching this rule that may be sent as events every minute. If more than <tt>rate</tt> matching log messages are received in a single minute, the extra messages are ignored. If rate is less than 0, the number of messages per minute is unlimited.</li><li><span class='code'>periodminutes</span>  - the number of minutes during which the publisher will only publish <tt>rate</tt> events or fewer. If not specified, the default is <strong>one minute</strong></li><li><span class='code'>threshold</span> - apply this rule only to messages with this log4j severity level or above. An example is \"WARN\" for warning level messages or higher.</li><li><span class='code'>content</span> - match only those messages whose contents match this regular expression.</li><li><span class='code'>exceptiontype</span> - match only those messages which are part of an exception message. The exception type must match this regular expression.</li></ul><br/><p>Example:<span class='code'>{\"alert\": false, \"rate\": 10, \"exceptiontype\": \"java.lang.StringIndexOutOfBoundsException\"}</span></p><p>This rule will send events to Cloudera Manager for every <span class='code'>StringIndexOutOfBoundsException</span>, up to a maximum of 10 every minute.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_sync_status_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Active NameNode Sync Status Health Check",
      "description" : "Enables the health check that verifies the active NameNode's sync status to the JournalNode",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalNode_java_opts",
      "required" : false,
      "default" : "",
      "displayName" : "Java Configuration Options for JournalNode",
      "description" : "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rlimit_fds",
      "required" : false,
      "displayName" : "Maximum Process File Descriptors",
      "description" : "If configured, overrides the process soft and hard rlimits (also called ulimits) for file descriptors to the configured value.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log4j_safety_valve",
      "required" : false,
      "displayName" : "JournalNode Logging Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_size",
      "required" : false,
      "default" : "200",
      "displayName" : "JournalNode Max Log Size",
      "description" : "The maximum size, in megabytes, per log file for JournalNode logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "Unexpected Exits Thresholds",
      "description" : "The health test thresholds for unexpected exits encountered within a recent period specified by the unexpected_exits_window configuration for the role.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "JOURNALNODE_role_env_safety_valve",
      "required" : false,
      "displayName" : "JournalNode Environment Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of this role except client configuration.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_frequency",
      "required" : false,
      "default" : "5.0",
      "displayName" : "Stacks Collection Frequency",
      "description" : "The frequency with which stacks are collected.",
      "relatedName" : "stacks_collection_frequency",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Log Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Log Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_web_metric_collection_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Web Metric Collection",
      "description" : "Enables the health test that the Cloudera Manager Agent can successfully contact and gather metrics from the web server.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Log Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_gc_duration_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Garbage Collection Duration Monitoring Period",
      "description" : "The period to review when computing the moving average of garbage collection time.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Dump Heap When Out of Memory",
      "description" : "When set, generates heap dump file when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_journalnode_http_port",
      "required" : false,
      "default" : "8480",
      "displayName" : "JournalNode HTTP Port",
      "description" : "Port for the JournalNode HTTP web UI. Combined with the JournalNode hostname to build its HTTP address.",
      "relatedName" : "dfs.journalnode.http-address",
      "validationState" : "OK"
    }, {
      "name" : "max_log_backup_index",
      "required" : false,
      "default" : "10",
      "displayName" : "JournalNode Maximum Log File Backups",
      "description" : "The maximum number of rolled log files to keep for JournalNode logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Heap Dump Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Unexpected Exits Monitoring Period",
      "description" : "The period to review when computing unexpected exits.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalNode_java_heapsize",
      "required" : false,
      "default" : "268435456",
      "displayName" : "Java Heap Size of JournalNode in Bytes",
      "description" : "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_method",
      "required" : false,
      "default" : "jstack",
      "displayName" : "Stacks Collection Method",
      "description" : "The method used to collect stacks. The jstack option involves periodically running the jstack command against the role's daemon process. The servlet method is available for those roles that have an HTTP server endpoint exposing the current stacks traces of all threads. When the servlet method is selected, that HTTP endpoint is periodically scraped.",
      "relatedName" : "stacks_collection_method",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_edits_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "JournalNode Edits Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's Directory on the local file system where NameNode edits are written..",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_bind_wildcard",
      "required" : false,
      "default" : "false",
      "displayName" : "Bind JournalNode to Wildcard Address",
      "description" : "If enabled, the JournalNode binds to the wildcard address (\"0.0.0.0\") on all of its ports.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_scm_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "JournalNode Process Health Test",
      "description" : "Enables the health test that the JournalNode's process state is consistent with the role configuration",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_sigkill_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Kill When Out of Memory",
      "description" : "When set, a SIGKILL signal is sent to the role process when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_journalnode_https_port",
      "required" : false,
      "default" : "8481",
      "displayName" : "Secure JournalNode Web UI Port (SSL)",
      "description" : "The base port where the secure JournalNode web UI listens.  Combined with the JournalNode's hostname to build its secure web UI address.",
      "relatedName" : "dfs.journalnode.https-address",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_hard_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Hard Limit",
      "description" : "Hard memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "rm_cpu_shares",
      "required" : false,
      "default" : "1024",
      "displayName" : "Cgroup CPU Shares",
      "description" : "Number of CPU shares to assign to this role. The greater the number of shares, the larger the share of the host's CPUs that will be given to this role when the host experiences CPU contention. Must be between 2 and 262144. Defaults to 1024 for processes not managed by Cloudera Manager.",
      "relatedName" : "cpu.shares",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_log_dir",
      "required" : false,
      "default" : "/var/log/hadoop-hdfs",
      "displayName" : "JournalNode Log Directory",
      "description" : "Directory where JournalNode will place its log files.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "jn_config_safety_valve",
      "required" : false,
      "displayName" : "JournalNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into <strong>hdfs-site.xml</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_directory",
      "required" : false,
      "displayName" : "Stacks Collection Directory",
      "description" : "The directory in which stacks logs are placed. If not set, stacks are logged into a <span class='code'>stacks</span> subdirectory of the role's log directory.",
      "relatedName" : "stacks_collection_directory",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_web_metric_collection_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"10000.0\"}",
      "displayName" : "Web Metric Collection Duration",
      "description" : "The health test thresholds on the duration of the metrics request to the web server.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_edits_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "JournalNode Edits Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's Directory on the local file system where NameNode edits are written.. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Directory on the local file system where NameNode edits are written. Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_io_weight",
      "required" : false,
      "default" : "500",
      "displayName" : "Cgroup I/O Weight",
      "description" : "Weight for the read I/O requests issued by this role. The greater the weight, the higher the priority of the requests when the host experiences I/O contention. Must be between 100 and 1000. Defaults to 1000 for processes not managed by Cloudera Manager.",
      "relatedName" : "blkio.weight",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_gc_duration_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"60.0\",\"warning\":\"30.0\"}",
      "displayName" : "Garbage Collection Duration Thresholds",
      "description" : "The health test thresholds for the weighted average time spent in Java garbage collection. Specified as a percentage of elapsed wall clock time.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_sync_status_startup_tolerance",
      "required" : false,
      "default" : "180",
      "displayName" : "Active NameNode Sync Status Startup Tolerance",
      "description" : "The amount of time at JournalNode startup allowed for the active NameNode to get in sync with the JournalNode.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_fsync_latency_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"3000.0\",\"warning\":\"1000.0\"}",
      "displayName" : "JournalNode Fsync Latency Thresholds",
      "description" : "The health test thresholds for JournalNode fsync latency.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "role_triggers",
      "required" : false,
      "default" : "[]",
      "displayName" : "Role Triggers",
      "description" : "<p>The configured triggers for this role. This is a JSON formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has all of the following fields:</p><ul><li><code>triggerName</code> <strong>(mandatory)</strong> - The name of the trigger. This value must be unique for the specific role. </li><li><code>triggerExpression</code> <strong>(mandatory)</strong> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <strong>(optional)</strong> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <strong> (optional)</strong> - By default set to 'true'. If set to 'false', the trigger will not be evaluated.</li><li><code>expressionEditorConfig</code> <strong> (optional)</strong> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here may lead to inconsistencies.</li></ul></p><p>For example, the following JSON formatted trigger configured for a DataNode fires if the DataNode has more than 1500 file-descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleName=$ROLENAME and last(fd_open) > 1500) DO health:bad\",\n  \"streamThreshold\": 0, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change in the future and, as a result, backward compatibility is not guaranteed between releases at this time.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "journalnode_host_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "JournalNode Host Health Test",
      "description" : "When computing the overall JournalNode health, consider the host's health.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "process_swap_memory_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"any\"}",
      "displayName" : "Process Swap Memory Thresholds",
      "description" : "The health test thresholds on the swap memory usage of the process.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_journalnode_rpc_port",
      "required" : false,
      "default" : "8485",
      "displayName" : "JournalNode RPC Port",
      "description" : "Port for the JournalNode's RPC. Combined with the JournalNode's hostname to build its RPC address.",
      "relatedName" : "dfs.journalnode.rpc-address",
      "validationState" : "OK"
    } ]
  }, {
    "roleType" : "NAMENODE",
    "items" : [ {
      "name" : "dfs_name_dir_list",
      "value" : "/data/dfs/nn,/data1/dfs/nn",
      "required" : true,
      "displayName" : "NameNode Data Directories",
      "description" : "Determines where on the local file system the NameNode should store the name table (fsimage). For redundancy, enter a comma-delimited list of directories to replicate the name table in all of the directories. Typical values are /data/N/dfs/nn where N=1..3.",
      "relatedName" : "dfs.namenode.name.dir",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_servicerpc_address",
      "value" : "8022",
      "required" : false,
      "displayName" : "NameNode Service RPC Port",
      "description" : "Optional port for the service-rpc address which can be used by HDFS daemons instead of sharing the RPC address used by the clients.",
      "relatedName" : "dfs.namenode.servicerpc-address",
      "validationState" : "OK"
    }, {
      "name" : "namenode_java_heapsize",
      "value" : "1610612736",
      "required" : false,
      "default" : "1073741824",
      "displayName" : "Java Heap Size of Namenode in Bytes",
      "description" : "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK",
      "validationMessage" : "Java Heap Size of Namenode in Bytes is at least 1GB for every million HDFS blocks. Suggested minimum value: 1073741824"
    }, {
      "name" : "namenode_rpc_latency_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5000.0\",\"warning\":\"1000.0\"}",
      "displayName" : "NameNode RPC Latency Thresholds",
      "description" : "The health check thresholds of the NameNode's RPC latency.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_rpc_latency_window",
      "required" : false,
      "default" : "5",
      "displayName" : "NameNode RPC Latency Monitoring Window",
      "description" : "The period to review when computing the moving average of the NameNode's RPC latency.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_thrift_threads_max",
      "required" : false,
      "default" : "20",
      "displayName" : "Hue Thrift Server Max Threadcount",
      "description" : "Maximum number of running threads for the Hue Thrift server running on the NameNode",
      "relatedName" : "dfs.thrift.threads.max",
      "validationState" : "OK"
    }, {
      "name" : "namenode_bind_wildcard",
      "required" : false,
      "default" : "false",
      "displayName" : "Bind NameNode to Wildcard Address",
      "description" : "If enabled, the NameNode binds to the wildcard address (\"0.0.0.0\") on all of its ports.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_http_port",
      "required" : false,
      "default" : "50070",
      "displayName" : "NameNode Web UI Port",
      "description" : "The base port where the DFS NameNode web UI listens. If the port number is 0, then the server starts on a free port. Combined with the NameNode's hostname to build its HTTP address.",
      "relatedName" : "dfs.namenode.http-address",
      "validationState" : "OK"
    }, {
      "name" : "log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "NameNode Logging Threshold",
      "description" : "The minimum log level for NameNode logs",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_data_retention",
      "required" : false,
      "default" : "104857600",
      "displayName" : "Stacks Collection Data Retention",
      "description" : "The amount of stacks data that is retained. After the retention limit is reached, the oldest data is deleted.",
      "relatedName" : "stacks_collection_data_retention",
      "validationState" : "OK"
    }, {
      "name" : "topology_script_file_name",
      "required" : false,
      "displayName" : "Topology Script File Name",
      "description" : "Full path to a custom topology script on the host file system. The topology script is used to determine the rack location of nodes. If left blank, a topology script will be provided that uses your hosts' rack information, visible in the \"Hosts\" page.",
      "relatedName" : "net.topology.script.file.name",
      "validationState" : "OK"
    }, {
      "name" : "dfs_name_dir_restore",
      "required" : false,
      "default" : "false",
      "displayName" : "Restore NameNode Directories at Checkpoint Time",
      "description" : "If set to false and if one of the replicas of the NameNode storage fails, such as temporarily failure of NFS, this directory is not used until the NameNode restarts.  If enabled, failed storage is re-checked on every checkpoint and, if it becomes valid, the NameNode will try to restore the edits and fsimage.",
      "relatedName" : "dfs.namenode.name.dir.restore",
      "validationState" : "OK"
    }, {
      "name" : "namenode_pause_duration_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"60.0\",\"warning\":\"30.0\"}",
      "displayName" : "Pause Duration Thresholds",
      "description" : "The health test thresholds for the weighted average extra time the pause monitor spent paused. Specified as a percentage of elapsed wall clock time.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Stacks Collection Enabled",
      "description" : "Whether or not periodic stacks collection is enabled.",
      "relatedName" : "stacks_collection_enabled",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_replication_max_streams",
      "required" : false,
      "default" : "20",
      "displayName" : "Maximum Number of Replication Threads on a DataNode",
      "description" : "The maximum number of outgoing replication threads a node can have at one time. This limit is waived for the highest priority replications. Configure dfs.namenode.replication.max-streams-hard-limit to set the absolute limit, including the highest-priority replications.",
      "relatedName" : "dfs.namenode.replication.max-streams",
      "validationState" : "OK"
    }, {
      "name" : "fs_trash_interval",
      "required" : false,
      "default" : "1440",
      "displayName" : "Filesystem Trash Interval",
      "description" : "Number of minutes between trash checkpoints. Also controls the number of minutes after which a trash checkpoint directory is deleted. To disable the trash feature, enter 0.",
      "relatedName" : "fs.trash.interval",
      "validationState" : "OK",
      "validationMessage" : "Trash checkpointing is on"
    }, {
      "name" : "namenode_host_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "NameNode Host Health Test",
      "description" : "When computing the overall NameNode health, consider the host's health.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_upgrade_status_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "HDFS Metadata Upgrade Status Health Test",
      "description" : "Enables the health test of the metadata upgrade status of the NameNode. This covers nonrolling metadata upgrades. Rolling metadata upgrades are covered in a separate health test.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_alerts",
      "required" : false,
      "default" : "true",
      "displayName" : "Enable Health Alerts for this Role",
      "description" : "When set, Cloudera Manager will send alerts when the health of this role reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_fd_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"70.0\",\"warning\":\"50.0\"}",
      "displayName" : "File Descriptor Monitoring Thresholds",
      "description" : "The health test thresholds of the number of file descriptors used. Specified as a percentage of file descriptor limit.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_directory_failures_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "NameNode Directory Failures Thresholds",
      "description" : "The health test thresholds of failed status directories in a NameNode.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_soft_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Soft Limit",
      "description" : "Soft memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process if and only if the host is facing memory pressure. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.soft_limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_quorum_journal_name",
      "required" : false,
      "displayName" : "Quorum-based Storage Journal name",
      "description" : "Name of the journal located on each JournalNode filesystem.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "process_auto_restart",
      "required" : false,
      "default" : "false",
      "displayName" : "Automatically Restart Process",
      "description" : "When set, this role's process is automatically (and transparently) restarted in the event of an unexpected failure.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_dir",
      "required" : false,
      "default" : "/tmp",
      "displayName" : "Heap Dump Directory",
      "description" : "Path to directory where heap dumps are generated when java.lang.OutOfMemoryError error is thrown. This directory is automatically created if it does not exist. If this directory already exists, role user must have write access to this directory. If this directory is shared among multiple roles, it should have 1777 permissions. The heap dump files are created with 600 permissions and are owned by the role user. The amount of free space in this directory should be greater than the maximum Java Process heap size configured for this role.",
      "relatedName" : "oom_heap_dump_dir",
      "validationState" : "OK"
    }, {
      "name" : "log_event_whitelist",
      "required" : false,
      "default" : "{\n  \"version\": \"0\",\n  \"rules\": [\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"FATAL\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Instead, use .*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Use .* instead\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.IOException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketClosedException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.EOFException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.nio.channels.CancelledKeyException\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 2, \"exceptiontype\": \".*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Unknown job [^ ]+ being deleted.*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Error executing shell command .+ No such process.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\".*attempt to override final parameter.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"[^ ]+ is a deprecated filesystem name. Use.*\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"WARN\"},\n    {\"alert\": false, \"rate\": 1, \"threshold\":\"INFO\", \"content\":\"Triggering checkpoint.*\"}\n  ]\n}\n",
      "displayName" : "Rules to Extract Events from Log Files",
      "description" : "<p>This file contains the rules which govern how log messages are turned into events by the custom log4j appender that this role loads. It is in JSON format, and is composed of a list of rules. Every log message is evaluated against each of these rules in turn to decide whether or not to send an event for that message.</p><p>Each rule has some or all of the following fields:</p><ul><li><span class='code'>alert</span> - whether or not events generated from this rule should be promoted to alerts. A value of \"true\" will cause alerts to be generated. If not specified, the default is \"false\".</li><li><span class='code'>rate</span> <strong>(mandatory)</strong> - the maximum number of log messages matching this rule that may be sent as events every minute. If more than <tt>rate</tt> matching log messages are received in a single minute, the extra messages are ignored. If rate is less than 0, the number of messages per minute is unlimited.</li><li><span class='code'>periodminutes</span>  - the number of minutes during which the publisher will only publish <tt>rate</tt> events or fewer. If not specified, the default is <strong>one minute</strong></li><li><span class='code'>threshold</span> - apply this rule only to messages with this log4j severity level or above. An example is \"WARN\" for warning level messages or higher.</li><li><span class='code'>content</span> - match only those messages whose contents match this regular expression.</li><li><span class='code'>exceptiontype</span> - match only those messages which are part of an exception message. The exception type must match this regular expression.</li></ul><br/><p>Example:<span class='code'>{\"alert\": false, \"rate\": 10, \"exceptiontype\": \"java.lang.StringIndexOutOfBoundsException\"}</span></p><p>This rule will send events to Cloudera Manager for every <span class='code'>StringIndexOutOfBoundsException</span>, up to a maximum of 10 every minute.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_shared_edits_dir",
      "required" : false,
      "displayName" : "Shared Edits Directory",
      "description" : "Directory on a shared storage device, such as a Quorum-based Storage URI or a local directory that is an NFS mount from a NAS, to store the NameNode edits. The value of this configuration is automatically generated to be the Quourm Journal URI if there are JournalNodes and this NameNode is Highly Available.",
      "relatedName" : "dfs.namenode.shared.edits.dir",
      "validationState" : "OK"
    }, {
      "name" : "dfs_thrift_threads_min",
      "required" : false,
      "default" : "10",
      "displayName" : "Hue Thrift Server Min Threadcount",
      "description" : "Minimum number of running threads for the Hue Thrift server running on the NameNode",
      "relatedName" : "dfs.thrift.threads.min",
      "validationState" : "OK"
    }, {
      "name" : "nameservice_mountpoints",
      "required" : false,
      "default" : "/",
      "displayName" : "Mountpoints",
      "description" : "Mountpoints that are mapped to this NameNode's Nameservice.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_edits_dir",
      "required" : false,
      "displayName" : "NameNode Edits Directories",
      "description" : "Directories on the local file system to store the NameNode edits. If not set, the edits are stored in the NameNode's Data Directories. The value of this configuration is automatically generated to be the Quorum-based Storage URI if there are JournalNodes and this NameNode is not Highly Available.",
      "relatedName" : "dfs.namenode.edits.dir",
      "validationState" : "OK"
    }, {
      "name" : "namenode_data_directories_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "NameNode Data Directories Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's Determines where on the local file system the NameNode should store the name table (fsimage). For redundancy, enter a comma-delimited list of directories to replicate the name table in all of the directories. Typical values are /data/N/dfs/nn where N=1..3.. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Determines where on the local file system the NameNode should store the name table (fsimage). For redundancy, enter a comma-delimited list of directories to replicate the name table in all of the directories. Typical values are /data/N/dfs/nn where N=1..3. Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_hosts_allow_safety_valve",
      "required" : false,
      "displayName" : "NameNode Advanced Configuration Snippet (Safety Valve) for dfs_hosts_allow.txt",
      "description" : "For advanced use only, a string to be inserted into <strong>dfs_hosts_allow.txt</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_config_safety_valve",
      "required" : false,
      "displayName" : "NameNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into <strong>hdfs-site.xml</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rlimit_fds",
      "required" : false,
      "displayName" : "Maximum Process File Descriptors",
      "description" : "If configured, overrides the process soft and hard rlimits (also called ulimits) for file descriptors to the configured value.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_replication_work_multiplier_per_iteration",
      "required" : false,
      "default" : "10",
      "displayName" : "Replication Work Multiplier Per Iteration",
      "description" : "This determines the total amount of block transfers to begin in parallel at a DataNode for replication, when such a command list is being sent over a DataNode heartbeat by the NameNode. The actual number is obtained by multiplying this value by the total number of live nodes in the cluster. The result number is the number of blocks to transfer immediately, per DataNode heartbeat.",
      "relatedName" : "dfs.namenode.replication.work.multiplier.per.iteration",
      "validationState" : "OK"
    }, {
      "name" : "log4j_safety_valve",
      "required" : false,
      "displayName" : "NameNode Logging Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_size",
      "required" : false,
      "default" : "200",
      "displayName" : "NameNode Max Log Size",
      "description" : "The maximum size, in megabytes, per log file for NameNode logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "Unexpected Exits Thresholds",
      "description" : "The health test thresholds for unexpected exits encountered within a recent period specified by the unexpected_exits_window configuration for the role.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_frequency",
      "required" : false,
      "default" : "5.0",
      "displayName" : "Stacks Collection Frequency",
      "description" : "The frequency with which stacks are collected.",
      "relatedName" : "stacks_collection_frequency",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Log Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Log Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_out_of_sync_journal_nodes_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "NameNode Out-Of-Sync JournalNodes Thresholds",
      "description" : "The health check thresholds for the number of out-of-sync JournalNodes for this NameNode.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Log Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Dump Heap When Out of Memory",
      "description" : "When set, generates heap dump file when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_backup_index",
      "required" : false,
      "default" : "10",
      "displayName" : "NameNode Maximum Log File Backups",
      "description" : "The maximum number of rolled log files to keep for NameNode logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "fs_checkpoint_period",
      "required" : false,
      "default" : "3600",
      "displayName" : "Filesystem Checkpoint Period",
      "description" : "The time between two periodic file system checkpoints.",
      "relatedName" : "dfs.namenode.checkpoint.period",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Unexpected Exits Monitoring Period",
      "description" : "The period to review when computing unexpected exits.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Heap Dump Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_method",
      "required" : false,
      "default" : "jstack",
      "displayName" : "Stacks Collection Method",
      "description" : "The method used to collect stacks. The jstack option involves periodically running the jstack command against the role's daemon process. The servlet method is available for those roles that have an HTTP server endpoint exposing the current stacks traces of all threads. When the servlet method is selected, that HTTP endpoint is periodically scraped.",
      "relatedName" : "stacks_collection_method",
      "validationState" : "OK"
    }, {
      "name" : "dfs_https_port",
      "required" : false,
      "default" : "50470",
      "displayName" : "Secure NameNode Web UI Port (SSL)",
      "description" : "The base port where the secure NameNode web UI listens.",
      "relatedName" : "dfs.https.port",
      "validationState" : "OK"
    }, {
      "name" : "namenode_blockstatechange_log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "NameNode Block State Change Logging Threshold",
      "description" : "The minimum log level for NameNode block state change log messages. Setting this to WARN or higher will greatly reduce the amount of log output related to block state changes.",
      "relatedName" : "log4j.logger.BlockStateChange",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_stale_datanode_interval",
      "required" : false,
      "default" : "30000",
      "displayName" : "Stale DataNode Time Interval",
      "description" : "Default time interval for marking a DataNode as \"stale\". If the NameNode has not received heartbeat messages from a DataNode for more than this time interval, the DataNode is marked and treated as \"stale\" by default.",
      "relatedName" : "dfs.namenode.stale.datanode.interval",
      "validationState" : "OK"
    }, {
      "name" : "namenode_checkpoint_age_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"400.0\",\"warning\":\"200.0\"}",
      "displayName" : "Filesystem Checkpoint Age Monitoring Thresholds",
      "description" : "The health test thresholds of the age of the HDFS namespace checkpoint. Specified as a percentage of the configured checkpoint interval.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_avoid_write_stale_datanode",
      "required" : false,
      "default" : "false",
      "displayName" : "Avoid Writing Stale DataNode",
      "description" : "Indicate whether or not to avoid writing to stale DataNodes for which heartbeat messages have not been received by the NameNode for more than Stale DataNode Time Interval. Writes avoid using stale DataNodes unless more than a configured ratio (dfs.namenode.write.stale.datanode.ratio) of DataNodes are marked as stale. See dfs.namenode.avoid.read.stale.datanode for a similar setting for reads.",
      "relatedName" : "dfs.namenode.avoid.write.stale.datanode",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_handler_count",
      "required" : false,
      "default" : "30",
      "displayName" : "NameNode Handler Count",
      "description" : "The number of server threads for the NameNode.",
      "relatedName" : "dfs.namenode.handler.count",
      "validationState" : "OK",
      "validationMessage" : "NameNode Handler Count is at least ln(number of datanodes) * 20. Suggested minimum value: 21"
    }, {
      "name" : "dfs_access_time_precision",
      "required" : false,
      "default" : "3600000",
      "displayName" : "Access Time Precision",
      "description" : "The access time for HDFS file is precise upto this value. Setting the value of 0 disables access times for HDFS. When using the NFS Gateway role, make sure this property is enabled.",
      "relatedName" : "dfs.access.time.precision",
      "validationState" : "OK"
    }, {
      "name" : "namenode_checkpoint_transactions_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"400.0\",\"warning\":\"200.0\"}",
      "displayName" : "Filesystem Checkpoint Transactions Monitoring Thresholds",
      "description" : "The health test thresholds of the number of transactions since the last HDFS namespace checkpoint. Specified as a percentage of the configured checkpointing transaction limit.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_safemode_threshold_pct",
      "required" : false,
      "default" : "0.999",
      "displayName" : "Safemode Threshold Percentage",
      "description" : "Specifies the percentage of blocks that should satisfy the minimal replication requirement defined by dfs.replication.min. Enter a value less than or equal to 0 to wait for any particular percentage of blocks before exiting safemode. Values greater than 1 will make safemode permanent.",
      "relatedName" : "dfs.namenode.safemode.threshold-pct",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_service_handler_count",
      "required" : false,
      "default" : "30",
      "displayName" : "NameNode Service Handler Count",
      "description" : "The number of server threads for the NameNode used for service calls. Only used when NameNode Service RPC Port is configured.",
      "relatedName" : "dfs.namenode.service.handler.count",
      "validationState" : "OK",
      "validationMessage" : "NameNode Service Handler Count is at least ln(number of datanodes) * 20. Suggested minimum value: 21"
    }, {
      "name" : "fs_checkpoint_txns",
      "required" : false,
      "default" : "1000000",
      "displayName" : "Filesystem Checkpoint Transaction Threshold",
      "description" : "The number of transactions after which the NameNode or SecondaryNameNode will create a checkpoint of the namespace, regardless of whether the checkpoint period has expired.",
      "relatedName" : "dfs.namenode.checkpoint.txns",
      "validationState" : "OK"
    }, {
      "name" : "dfs_thrift_timeout",
      "required" : false,
      "default" : "60",
      "displayName" : "Hue Thrift Server Timeout",
      "description" : "Timeout in seconds for the Hue Thrift server running on the NameNode",
      "relatedName" : "dfs.thrift.timeout",
      "validationState" : "OK"
    }, {
      "name" : "namenode_data_directories_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "NameNode Data Directories Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's Determines where on the local file system the NameNode should store the name table (fsimage). For redundancy, enter a comma-delimited list of directories to replicate the name table in all of the directories. Typical values are /data/N/dfs/nn where N=1..3..",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_safe_mode_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "NameNode Safemode Health Test",
      "description" : "Enables the health test that the NameNode is not in safemode",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_federation_namenode_nameservice",
      "required" : false,
      "displayName" : "NameNode Nameservice",
      "description" : "Nameservice of this NameNode. The Nameservice represents the interface to this NameNode and its High Availability partner. The Nameservice also represents the namespace associated with a federated NameNode.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_sigkill_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Kill When Out of Memory",
      "description" : "When set, a SIGKILL signal is sent to the role process when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_safemode_min_datanodes",
      "required" : false,
      "default" : "0",
      "displayName" : "Safemode Minimum DataNodes",
      "description" : "Specifies the number of DataNodes that must be  live before the name node exits safemode. Enter a value less than or equal to 0 to take the number of live DataNodes into account when deciding whether to remain in safemode during startup. Values greater than the number of DataNodes in the cluster will make safemode permanent.",
      "relatedName" : "dfs.safemode.min.datanodes",
      "validationState" : "OK"
    }, {
      "name" : "namenode_hosts_exclude_safety_valve",
      "required" : false,
      "displayName" : "NameNode Advanced Configuration Snippet (Safety Valve) for dfs_hosts_exclude.txt",
      "description" : "For advanced use only, a string to be inserted into <strong>dfs_hosts_exclude.txt</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_hard_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Hard Limit",
      "description" : "Hard memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "rm_cpu_shares",
      "required" : false,
      "default" : "1024",
      "displayName" : "Cgroup CPU Shares",
      "description" : "Number of CPU shares to assign to this role. The greater the number of shares, the larger the share of the host's CPUs that will be given to this role when the host experiences CPU contention. Must be between 2 and 262144. Defaults to 1024 for processes not managed by Cloudera Manager.",
      "relatedName" : "cpu.shares",
      "validationState" : "OK"
    }, {
      "name" : "namenode_web_metric_collection_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Web Metric Collection",
      "description" : "Enables the health test that the Cloudera Manager Agent can successfully contact and gather metrics from the web server.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_directory",
      "required" : false,
      "displayName" : "Stacks Collection Directory",
      "description" : "The directory in which stacks logs are placed. If not set, stacks are logged into a <span class='code'>stacks</span> subdirectory of the role's log directory.",
      "relatedName" : "stacks_collection_directory",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_web_metric_collection_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"10000.0\"}",
      "displayName" : "Web Metric Collection Duration",
      "description" : "The health test thresholds on the duration of the metrics request to the web server.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_startup_tolerance",
      "required" : false,
      "default" : "5",
      "displayName" : "Health Check Startup Tolerance",
      "description" : "The amount of time allowed after this role is started that failures of health checks that rely on communication with this role will be tolerated.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_log_dir",
      "required" : false,
      "default" : "/var/log/hadoop-hdfs",
      "displayName" : "NameNode Log Directory",
      "description" : "Directory where NameNode will place its log files.",
      "relatedName" : "hadoop.log.dir",
      "validationState" : "OK"
    }, {
      "name" : "rm_io_weight",
      "required" : false,
      "default" : "500",
      "displayName" : "Cgroup I/O Weight",
      "description" : "Weight for the read I/O requests issued by this role. The greater the weight, the higher the priority of the requests when the host experiences I/O contention. Must be between 100 and 1000. Defaults to 1000 for processes not managed by Cloudera Manager.",
      "relatedName" : "blkio.weight",
      "validationState" : "OK"
    }, {
      "name" : "hadoop_metrics2_safety_valve",
      "required" : false,
      "displayName" : "Hadoop Metrics2 Advanced Configuration Snippet (Safety Valve)",
      "description" : "Advanced Configuration Snippet (Safety Valve) for Hadoop Metrics2. Properties will be inserted into <strong>hadoop-metrics2.properties</strong>.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_java_opts",
      "required" : false,
      "default" : "-XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled",
      "displayName" : "Java Configuration Options for NameNode",
      "description" : "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "NAMENODE_role_env_safety_valve",
      "required" : false,
      "displayName" : "NameNode Environment Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of this role except client configuration.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "autofailover_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Automatic Failover",
      "description" : "Enable Automatic Failover to maintain High Availability. Requires a ZooKeeper service and a High Availability NameNode partner.",
      "relatedName" : "dfs.ha.automatic-failover.enabled",
      "validationState" : "OK"
    }, {
      "name" : "namenode_pause_duration_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Pause Duration Monitoring Period",
      "description" : "The period to review when computing the moving average of extra time the pause monitor spent paused.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_invalidate_work_pct_per_iteration",
      "required" : false,
      "default" : "0.32",
      "displayName" : "Invalidate Work Percentage Per Iteration",
      "description" : "This determines the percentage amount of block invalidations (deletes) to do over a single DataNode heartbeat deletion command. The final deletion count is determined by applying this percentage to the number of live nodes in the system. The resultant number is the number of blocks from the deletion list chosen for proper invalidation over a single heartbeat of a single DataNode.",
      "relatedName" : "dfs.namenode.invalidate.work.pct.per.iteration",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_write_stale_datanode_ratio",
      "required" : false,
      "default" : "0.5",
      "displayName" : "Write Stale DataNode Ratio",
      "description" : "When the ratio of number stale DataNodes to total DataNodes marked is greater than this ratio, permit writing to stale nodes to prevent causing hotspots.",
      "relatedName" : "dfs.namenode.write.stale.datanode.ratio",
      "validationState" : "OK"
    }, {
      "name" : "namenode_rolling_upgrade_status_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "HDFS Rolling Metadata Upgrade Status Health Test",
      "description" : "Enables the health test of the rolling metadata upgrade status of the NameNode. This covers rolling metadata upgrades. Nonrolling metadata upgrades are covered in a separate health test.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "role_triggers",
      "required" : false,
      "default" : "[]",
      "displayName" : "Role Triggers",
      "description" : "<p>The configured triggers for this role. This is a JSON formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has all of the following fields:</p><ul><li><code>triggerName</code> <strong>(mandatory)</strong> - The name of the trigger. This value must be unique for the specific role. </li><li><code>triggerExpression</code> <strong>(mandatory)</strong> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <strong>(optional)</strong> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <strong> (optional)</strong> - By default set to 'true'. If set to 'false', the trigger will not be evaluated.</li><li><code>expressionEditorConfig</code> <strong> (optional)</strong> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here may lead to inconsistencies.</li></ul></p><p>For example, the following JSON formatted trigger configured for a DataNode fires if the DataNode has more than 1500 file-descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleName=$ROLENAME and last(fd_open) > 1500) DO health:bad\",\n  \"streamThreshold\": 0, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change in the future and, as a result, backward compatibility is not guaranteed between releases at this time.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_replication_max_streams_hard_limit",
      "required" : false,
      "default" : "40",
      "displayName" : "Hard Limit on the Number of Replication Threads on a Datanode",
      "description" : "The absolute maximum number of outgoing replication threads a given node can have at one time. The regular limit (dfs.namenode.replication.max-streams) is waived for highest-priority block replications. Highest replication priority is for blocks that are at a very high risk of loss if the disk or server on which they remain fails. These are usually blocks with only one copy, or blocks with zero live copies but a copy in a node being decommissioned. dfs.namenode.replication.max-streams-hard-limit provides a limit on the total number of outgoing replication threads, including threads of all priorities.",
      "relatedName" : "dfs.namenode.replication.max-streams-hard-limit",
      "validationState" : "OK"
    }, {
      "name" : "process_swap_memory_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"any\"}",
      "displayName" : "Process Swap Memory Thresholds",
      "description" : "The health test thresholds on the swap memory usage of the process.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_scm_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "NameNode Process Health Test",
      "description" : "Enables the health test that the NameNode's process state is consistent with the role configuration",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "namenode_port",
      "required" : false,
      "default" : "8020",
      "displayName" : "NameNode Port",
      "description" : "The port where the NameNode runs the HDFS protocol. Combined with the NameNode's hostname to build its address.",
      "relatedName" : "fs.defaultFS",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_avoid_read_stale_datanode",
      "required" : false,
      "default" : "false",
      "displayName" : "Avoid Reading Stale DataNode",
      "description" : "Indicate whether or not to avoid reading from stale DataNodes for which heartbeat messages have not been received by the NameNode for more than Stale DataNode Time Interval. Stale DataNodes are moved to the end of the node list returned for reading. See dfs.namenode.avoid.write.stale.datanode for a similar setting for writes.",
      "relatedName" : "dfs.namenode.avoid.read.stale.datanode",
      "validationState" : "OK"
    }, {
      "name" : "dfs_safemode_extension",
      "required" : false,
      "default" : "30000",
      "displayName" : "Safemode Extension",
      "description" : "Determines extension of safemode in milliseconds after the threshold level is reached.",
      "relatedName" : "dfs.namenode.safemode.extension",
      "validationState" : "OK"
    }, {
      "name" : "dfs_namenode_plugins_list",
      "required" : false,
      "default" : "",
      "displayName" : "NameNode Plugins",
      "description" : "Comma-separated list of NameNode plug-ins to be activated. If one plug-in cannot be loaded, all the plug-ins are ignored.",
      "relatedName" : "dfs.namenode.plugins",
      "validationState" : "OK"
    } ]
  }, {
    "roleType" : "NFSGATEWAY",
    "items" : [ {
      "name" : "dfs_nfs_exports_allowed_hosts",
      "required" : false,
      "default" : "* rw",
      "displayName" : "Allowed Hosts and Privileges",
      "description" : "By default, NFS Gateway exported directories can be mounted by any client. For better access control, update this property with a list of host names and access privileges separated by whitespace characters. Host name format can be a single host, a Java regular expression, or an IPv4 address. The access privilege uses <strong>rw</strong> to specify readwrite and <strong>ro</strong> to specify readonly access. If the access privilege is not provided, the default is read-only. Examples of host name format and access privilege: \"192.168.0.0/22 rw\", \"host.*.example.com\", \"host1.test.org ro\".",
      "relatedName" : "dfs.nfs.exports.allowed.hosts",
      "validationState" : "OK"
    }, {
      "name" : "log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "NFS Gateway Logging Threshold",
      "description" : "The minimum log level for NFS Gateway logs",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_data_retention",
      "required" : false,
      "default" : "104857600",
      "displayName" : "Stacks Collection Data Retention",
      "description" : "The amount of stacks data that is retained. After the retention limit is reached, the oldest data is deleted.",
      "relatedName" : "stacks_collection_data_retention",
      "validationState" : "OK"
    }, {
      "name" : "nfsgateway_java_opts",
      "required" : false,
      "default" : "",
      "displayName" : "Java Configuration Options for NFS Gateway",
      "description" : "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Stacks Collection Enabled",
      "description" : "Whether or not periodic stacks collection is enabled.",
      "relatedName" : "stacks_collection_enabled",
      "validationState" : "OK"
    }, {
      "name" : "dfs_nfs3_dump_dir",
      "required" : false,
      "default" : "/tmp/.hdfs-nfs",
      "displayName" : "Temporary Dump Directory",
      "description" : "NFS clients often reorder writes. As a result, sequential writes can arrive at the NFS Gateway in random order. This directory is used to temporarily save out-of-order writes before writing to HDFS. For each file, the out-of-order writes are dumped after they are accumulated to exceed certain threshold (e.g., 1MB) in memory. Please make sure this directory has enough space. For example, if the application uploads 10 files with each having 100MB, it is recommended that this directory have roughly 1GB of space in case write reorder happens (in the worst case) to every file.",
      "relatedName" : "dfs.nfs3.dump.dir",
      "validationState" : "OK"
    }, {
      "name" : "oom_sigkill_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Kill When Out of Memory",
      "description" : "When set, a SIGKILL signal is sent to the role process when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "NFSGATEWAY_role_env_safety_valve",
      "required" : false,
      "displayName" : "NFS Gateway Environment Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of this role except client configuration.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "nfsgateway_log_dir",
      "required" : false,
      "default" : "/var/log/hadoop-hdfs",
      "displayName" : "NFS Gateway Log Directory",
      "description" : "Directory where NFS Gateway will place its log files.",
      "relatedName" : "hadoop.log.dir",
      "validationState" : "OK"
    }, {
      "name" : "nfs3_mountd_port",
      "required" : false,
      "default" : "4242",
      "displayName" : "NFS Gateway MountD Port",
      "description" : "The port number of the mount daemon implemented inside the NFS Gateway server role.",
      "relatedName" : "nfs3.mountd.port",
      "validationState" : "OK"
    }, {
      "name" : "nfs3_portmap_port",
      "required" : false,
      "default" : "111",
      "displayName" : "Portmap (or Rpcbind) Port",
      "description" : "The port number of the system portmap or rpcbind service. This configuration is used by Cloudera Manager to verify if the system portmap or rpcbind service is running before starting NFS Gateway role. Cloudera Manager does not manage the system portmap or rpcbind service.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_alerts",
      "required" : false,
      "default" : "true",
      "displayName" : "Enable Health Alerts for this Role",
      "description" : "When set, Cloudera Manager will send alerts when the health of this role reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "nfsgateway_java_heapsize",
      "required" : false,
      "default" : "268435456",
      "displayName" : "Java Heap Size of NFS Gateway in Bytes",
      "description" : "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_hard_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Hard Limit",
      "description" : "Hard memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_soft_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Soft Limit",
      "description" : "Soft memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process if and only if the host is facing memory pressure. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.soft_limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "process_auto_restart",
      "required" : false,
      "default" : "false",
      "displayName" : "Automatically Restart Process",
      "description" : "When set, this role's process is automatically (and transparently) restarted in the event of an unexpected failure.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_cpu_shares",
      "required" : false,
      "default" : "1024",
      "displayName" : "Cgroup CPU Shares",
      "description" : "Number of CPU shares to assign to this role. The greater the number of shares, the larger the share of the host's CPUs that will be given to this role when the host experiences CPU contention. Must be between 2 and 262144. Defaults to 1024 for processes not managed by Cloudera Manager.",
      "relatedName" : "cpu.shares",
      "validationState" : "OK"
    }, {
      "name" : "log_event_whitelist",
      "required" : false,
      "default" : "{\n  \"version\": \"0\",\n  \"rules\": [\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"FATAL\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 2, \"exceptiontype\": \".*\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"WARN\"}\n  ]\n}",
      "displayName" : "Rules to Extract Events from Log Files",
      "description" : "<p>This file contains the rules which govern how log messages are turned into events by the custom log4j appender that this role loads. It is in JSON format, and is composed of a list of rules. Every log message is evaluated against each of these rules in turn to decide whether or not to send an event for that message.</p><p>Each rule has some or all of the following fields:</p><ul><li><span class='code'>alert</span> - whether or not events generated from this rule should be promoted to alerts. A value of \"true\" will cause alerts to be generated. If not specified, the default is \"false\".</li><li><span class='code'>rate</span> <strong>(mandatory)</strong> - the maximum number of log messages matching this rule that may be sent as events every minute. If more than <tt>rate</tt> matching log messages are received in a single minute, the extra messages are ignored. If rate is less than 0, the number of messages per minute is unlimited.</li><li><span class='code'>periodminutes</span>  - the number of minutes during which the publisher will only publish <tt>rate</tt> events or fewer. If not specified, the default is <strong>one minute</strong></li><li><span class='code'>threshold</span> - apply this rule only to messages with this log4j severity level or above. An example is \"WARN\" for warning level messages or higher.</li><li><span class='code'>content</span> - match only those messages whose contents match this regular expression.</li><li><span class='code'>exceptiontype</span> - match only those messages which are part of an exception message. The exception type must match this regular expression.</li></ul><br/><p>Example:<span class='code'>{\"alert\": false, \"rate\": 10, \"exceptiontype\": \"java.lang.StringIndexOutOfBoundsException\"}</span></p><p>This rule will send events to Cloudera Manager for every <span class='code'>StringIndexOutOfBoundsException</span>, up to a maximum of 10 every minute.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_dir",
      "required" : false,
      "default" : "/tmp",
      "displayName" : "Heap Dump Directory",
      "description" : "Path to directory where heap dumps are generated when java.lang.OutOfMemoryError error is thrown. This directory is automatically created if it does not exist. If this directory already exists, role user must have write access to this directory. If this directory is shared among multiple roles, it should have 1777 permissions. The heap dump files are created with 600 permissions and are owned by the role user. The amount of free space in this directory should be greater than the maximum Java Process heap size configured for this role.",
      "relatedName" : "oom_heap_dump_dir",
      "validationState" : "OK"
    }, {
      "name" : "nfs3_server_port",
      "required" : false,
      "default" : "2049",
      "displayName" : "NFS Gateway Server Port",
      "description" : "The NFS Gateway server port.",
      "relatedName" : "nfs3.server.port",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_directory",
      "required" : false,
      "displayName" : "Stacks Collection Directory",
      "description" : "The directory in which stacks logs are placed. If not set, stacks are logged into a <span class='code'>stacks</span> subdirectory of the role's log directory.",
      "relatedName" : "stacks_collection_directory",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_io_weight",
      "required" : false,
      "default" : "500",
      "displayName" : "Cgroup I/O Weight",
      "description" : "Weight for the read I/O requests issued by this role. The greater the weight, the higher the priority of the requests when the host experiences I/O contention. Must be between 100 and 1000. Defaults to 1000 for processes not managed by Cloudera Manager.",
      "relatedName" : "blkio.weight",
      "validationState" : "OK"
    }, {
      "name" : "rlimit_fds",
      "required" : false,
      "displayName" : "Maximum Process File Descriptors",
      "description" : "If configured, overrides the process soft and hard rlimits (also called ulimits) for file descriptors to the configured value.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log4j_safety_valve",
      "required" : false,
      "displayName" : "NFS Gateway Logging Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_size",
      "required" : false,
      "default" : "200",
      "displayName" : "NFS Gateway Max Log Size",
      "description" : "The maximum size, in megabytes, per log file for NFS Gateway logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "nfsgateway_scm_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "NFS Gateway Process Health Test",
      "description" : "Enables the health test that the NFS Gateway's process state is consistent with the role configuration",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "nfsgateway_config_safety_valve",
      "required" : false,
      "displayName" : "NFS Gateway Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into <strong>hdfs-site.xml</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "Unexpected Exits Thresholds",
      "description" : "The health test thresholds for unexpected exits encountered within a recent period specified by the unexpected_exits_window configuration for the role.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_frequency",
      "required" : false,
      "default" : "5.0",
      "displayName" : "Stacks Collection Frequency",
      "description" : "The frequency with which stacks are collected.",
      "relatedName" : "stacks_collection_frequency",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Log Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Log Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "role_triggers",
      "required" : false,
      "default" : "[]",
      "displayName" : "Role Triggers",
      "description" : "<p>The configured triggers for this role. This is a JSON formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has all of the following fields:</p><ul><li><code>triggerName</code> <strong>(mandatory)</strong> - The name of the trigger. This value must be unique for the specific role. </li><li><code>triggerExpression</code> <strong>(mandatory)</strong> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <strong>(optional)</strong> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <strong> (optional)</strong> - By default set to 'true'. If set to 'false', the trigger will not be evaluated.</li><li><code>expressionEditorConfig</code> <strong> (optional)</strong> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here may lead to inconsistencies.</li></ul></p><p>For example, the following JSON formatted trigger configured for a DataNode fires if the DataNode has more than 1500 file-descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleName=$ROLENAME and last(fd_open) > 1500) DO health:bad\",\n  \"streamThreshold\": 0, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change in the future and, as a result, backward compatibility is not guaranteed between releases at this time.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "process_swap_memory_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"any\"}",
      "displayName" : "Process Swap Memory Thresholds",
      "description" : "The health test thresholds on the swap memory usage of the process.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Log Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "nfsgateway_fd_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"70.0\",\"warning\":\"50.0\"}",
      "displayName" : "File Descriptor Monitoring Thresholds",
      "description" : "The health test thresholds of the number of file descriptors used. Specified as a percentage of file descriptor limit.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Dump Heap When Out of Memory",
      "description" : "When set, generates heap dump file when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_backup_index",
      "required" : false,
      "default" : "10",
      "displayName" : "NFS Gateway Maximum Log File Backups",
      "description" : "The maximum number of rolled log files to keep for NFS Gateway logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Unexpected Exits Monitoring Period",
      "description" : "The period to review when computing unexpected exits.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Heap Dump Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "nfsgateway_host_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "NFS Gateway Host Health Test",
      "description" : "When computing the overall NFS Gateway health, consider the host's health.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_method",
      "required" : false,
      "default" : "jstack",
      "displayName" : "Stacks Collection Method",
      "description" : "The method used to collect stacks. The jstack option involves periodically running the jstack command against the role's daemon process. The servlet method is available for those roles that have an HTTP server endpoint exposing the current stacks traces of all threads. When the servlet method is selected, that HTTP endpoint is periodically scraped.",
      "relatedName" : "stacks_collection_method",
      "validationState" : "OK"
    } ]
  }, {
    "roleType" : "SECONDARYNAMENODE",
    "items" : [ {
      "name" : "fs_checkpoint_dir_list",
      "value" : "/data/dfs/snn",
      "required" : true,
      "displayName" : "HDFS Checkpoint Directories",
      "description" : "Determines where on the local file system the DFS SecondaryNameNode should store the temporary images to merge. For redundancy, enter a comma-delimited list of directories to replicate the image in all of the directories. Typical values are /data/N/dfs/snn for N = 1, 2, 3...",
      "relatedName" : "dfs.namenode.checkpoint.dir",
      "validationState" : "OK"
    }, {
      "name" : "secondary_namenode_java_heapsize",
      "value" : "1610612736",
      "required" : false,
      "default" : "1073741824",
      "displayName" : "Java Heap Size of Secondary namenode in Bytes",
      "description" : "Maximum size in bytes for the Java Process heap memory.  Passed to Java -Xmx.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_data_retention",
      "required" : false,
      "default" : "104857600",
      "displayName" : "Stacks Collection Data Retention",
      "description" : "The amount of stacks data that is retained. After the retention limit is reached, the oldest data is deleted.",
      "relatedName" : "stacks_collection_data_retention",
      "validationState" : "OK"
    }, {
      "name" : "log_threshold",
      "required" : false,
      "default" : "INFO",
      "displayName" : "SecondaryNameNode Logging Threshold",
      "description" : "The minimum log level for SecondaryNameNode logs",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Stacks Collection Enabled",
      "description" : "Whether or not periodic stacks collection is enabled.",
      "relatedName" : "stacks_collection_enabled",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_config_safety_valve",
      "required" : false,
      "displayName" : "SecondaryNameNode Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
      "description" : "For advanced use only, a string to be inserted into <strong>hdfs-site.xml</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "enable_alerts",
      "required" : false,
      "default" : "true",
      "displayName" : "Enable Health Alerts for this Role",
      "description" : "When set, Cloudera Manager will send alerts when the health of this role reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_soft_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Soft Limit",
      "description" : "Soft memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process if and only if the host is facing memory pressure. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.soft_limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "process_auto_restart",
      "required" : false,
      "default" : "false",
      "displayName" : "Automatically Restart Process",
      "description" : "When set, this role's process is automatically (and transparently) restarted in the event of an unexpected failure.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_dir",
      "required" : false,
      "default" : "/tmp",
      "displayName" : "Heap Dump Directory",
      "description" : "Path to directory where heap dumps are generated when java.lang.OutOfMemoryError error is thrown. This directory is automatically created if it does not exist. If this directory already exists, role user must have write access to this directory. If this directory is shared among multiple roles, it should have 1777 permissions. The heap dump files are created with 600 permissions and are owned by the role user. The amount of free space in this directory should be greater than the maximum Java Process heap size configured for this role.",
      "relatedName" : "oom_heap_dump_dir",
      "validationState" : "OK"
    }, {
      "name" : "log_event_whitelist",
      "required" : false,
      "default" : "{\n  \"version\": \"0\",\n  \"rules\": [\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"FATAL\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Instead, use .*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\": \".* is deprecated. Use .* instead\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.IOException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.net.SocketClosedException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.io.EOFException\"},\n    {\"alert\": false, \"rate\": 0, \"exceptiontype\": \"java.nio.channels.CancelledKeyException\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 2, \"exceptiontype\": \".*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Unknown job [^ ]+ being deleted.*\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"Error executing shell command .+ No such process.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\".*attempt to override final parameter.+\"},\n    {\"alert\": false, \"rate\": 0, \"threshold\":\"WARN\", \"content\":\"[^ ]+ is a deprecated filesystem name. Use.*\"},\n    {\"alert\": false, \"rate\": 1, \"periodminutes\": 1, \"threshold\":\"WARN\"},\n    {\"alert\": false, \"rate\": 1, \"threshold\":\"INFO\", \"content\":\"Triggering checkpoint.*\"}\n  ]\n}\n",
      "displayName" : "Rules to Extract Events from Log Files",
      "description" : "<p>This file contains the rules which govern how log messages are turned into events by the custom log4j appender that this role loads. It is in JSON format, and is composed of a list of rules. Every log message is evaluated against each of these rules in turn to decide whether or not to send an event for that message.</p><p>Each rule has some or all of the following fields:</p><ul><li><span class='code'>alert</span> - whether or not events generated from this rule should be promoted to alerts. A value of \"true\" will cause alerts to be generated. If not specified, the default is \"false\".</li><li><span class='code'>rate</span> <strong>(mandatory)</strong> - the maximum number of log messages matching this rule that may be sent as events every minute. If more than <tt>rate</tt> matching log messages are received in a single minute, the extra messages are ignored. If rate is less than 0, the number of messages per minute is unlimited.</li><li><span class='code'>periodminutes</span>  - the number of minutes during which the publisher will only publish <tt>rate</tt> events or fewer. If not specified, the default is <strong>one minute</strong></li><li><span class='code'>threshold</span> - apply this rule only to messages with this log4j severity level or above. An example is \"WARN\" for warning level messages or higher.</li><li><span class='code'>content</span> - match only those messages whose contents match this regular expression.</li><li><span class='code'>exceptiontype</span> - match only those messages which are part of an exception message. The exception type must match this regular expression.</li></ul><br/><p>Example:<span class='code'>{\"alert\": false, \"rate\": 10, \"exceptiontype\": \"java.lang.StringIndexOutOfBoundsException\"}</span></p><p>This rule will send events to Cloudera Manager for every <span class='code'>StringIndexOutOfBoundsException</span>, up to a maximum of 10 every minute.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_secondary_http_port",
      "required" : false,
      "default" : "50090",
      "displayName" : "SecondaryNameNode Web UI Port",
      "description" : "The SecondaryNameNode HTTP port. If the port is 0, then the server starts on a free port. Combined with the SecondaryNameNode's hostname to build its HTTP address.",
      "relatedName" : "dfs.namenode.secondary.http-address",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_host_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "SecondaryNameNode Host Health Test",
      "description" : "When computing the overall SecondaryNameNode health, consider the host's health.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_log_dir",
      "required" : false,
      "default" : "/var/log/hadoop-hdfs",
      "displayName" : "SecondaryNameNode Log Directory",
      "description" : "Directory where SecondaryNameNode will place its log files.",
      "relatedName" : "hadoop.log.dir",
      "validationState" : "OK"
    }, {
      "name" : "rlimit_fds",
      "required" : false,
      "displayName" : "Maximum Process File Descriptors",
      "description" : "If configured, overrides the process soft and hard rlimits (also called ulimits) for file descriptors to the configured value.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log4j_safety_valve",
      "required" : false,
      "displayName" : "SecondaryNameNode Logging Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, a string to be inserted into <strong>log4j.properties</strong> for this role only.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_size",
      "required" : false,
      "default" : "200",
      "displayName" : "SecondaryNameNode Max Log Size",
      "description" : "The maximum size, in megabytes, per log file for SecondaryNameNode logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
      "displayName" : "Unexpected Exits Thresholds",
      "description" : "The health test thresholds for unexpected exits encountered within a recent period specified by the unexpected_exits_window configuration for the role.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_checkpoint_directories_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "HDFS Checkpoint Directories Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's Determines where on the local file system the DFS SecondaryNameNode should store the temporary images to merge. For redundancy, enter a comma-delimited list of directories to replicate the image in all of the directories. Typical values are /data/N/dfs/snn for N = 1, 2, 3.... Specified as a percentage of the capacity on that filesystem. This setting is not used if a Determines where on the local file system the DFS SecondaryNameNode should store the temporary images to merge. For redundancy, enter a comma-delimited list of directories to replicate the image in all of the directories. Typical values are /data/N/dfs/snn for N = 1, 2, 3... Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_frequency",
      "required" : false,
      "default" : "5.0",
      "displayName" : "Stacks Collection Frequency",
      "description" : "The frequency with which stacks are collected.",
      "relatedName" : "stacks_collection_frequency",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Log Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Log Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_secondarynamenode_nameservice",
      "required" : false,
      "displayName" : "SecondaryNameNode Nameservice",
      "description" : "Nameservice of this SecondaryNameNode",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "log_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Log Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's log directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_heap_dump_enabled",
      "required" : false,
      "default" : "false",
      "displayName" : "Dump Heap When Out of Memory",
      "description" : "When set, generates heap dump file when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "max_log_backup_index",
      "required" : false,
      "default" : "10",
      "displayName" : "SecondaryNameNode Maximum Log File Backups",
      "description" : "The maximum number of rolled log files to keep for SecondaryNameNode logs.  Typically used by log4j or logback.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondary_namenode_bind_wildcard",
      "required" : false,
      "default" : "false",
      "displayName" : "Bind SecondaryNameNode to Wildcard Address",
      "description" : "If enabled, the SecondaryNameNode binds to the wildcard address (\"0.0.0.0\") on all of its ports.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "fs_checkpoint_period",
      "required" : false,
      "default" : "3600",
      "displayName" : "Filesystem Checkpoint Period",
      "description" : "The time between two periodic file system checkpoints.",
      "relatedName" : "dfs.namenode.checkpoint.period",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_percentage_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"never\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Percentage Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory. Specified as a percentage of the capacity on that filesystem. This setting is not used if a Heap Dump Directory Free Space Monitoring Absolute Thresholds setting is configured.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "unexpected_exits_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Unexpected Exits Monitoring Period",
      "description" : "The period to review when computing unexpected exits.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_method",
      "required" : false,
      "default" : "jstack",
      "displayName" : "Stacks Collection Method",
      "description" : "The method used to collect stacks. The jstack option involves periodically running the jstack command against the role's daemon process. The servlet method is available for those roles that have an HTTP server endpoint exposing the current stacks traces of all threads. When the servlet method is selected, that HTTP endpoint is periodically scraped.",
      "relatedName" : "stacks_collection_method",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_scm_health_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "SecondaryNameNode Process Health Test",
      "description" : "Enables the health test that the SecondaryNameNode's process state is consistent with the role configuration",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_gc_duration_window",
      "required" : false,
      "default" : "5",
      "displayName" : "Garbage Collection Duration Monitoring Period",
      "description" : "The period to review when computing the moving average of garbage collection time.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_checkpoint_directories_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "HDFS Checkpoint Directories Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's Determines where on the local file system the DFS SecondaryNameNode should store the temporary images to merge. For redundancy, enter a comma-delimited list of directories to replicate the image in all of the directories. Typical values are /data/N/dfs/snn for N = 1, 2, 3....",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_java_opts",
      "required" : false,
      "default" : "-XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:-CMSConcurrentMTEnabled -XX:CMSInitiatingOccupancyFraction=70 -XX:+CMSParallelRemarkEnabled",
      "displayName" : "Java Configuration Options for Secondary NameNode",
      "description" : "These arguments will be passed as part of the Java command line. Commonly, garbage collection flags or extra debugging flags would be passed here.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "fs_checkpoint_txns",
      "required" : false,
      "default" : "1000000",
      "displayName" : "Filesystem Checkpoint Transaction Threshold",
      "description" : "The number of transactions after which the NameNode or SecondaryNameNode will create a checkpoint of the namespace, regardless of whether the checkpoint period has expired.",
      "relatedName" : "dfs.namenode.checkpoint.txns",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_gc_duration_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"60.0\",\"warning\":\"30.0\"}",
      "displayName" : "Garbage Collection Duration Thresholds",
      "description" : "The health test thresholds for the weighted average time spent in Java garbage collection. Specified as a percentage of elapsed wall clock time.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "oom_sigkill_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Kill When Out of Memory",
      "description" : "When set, a SIGKILL signal is sent to the role process when java.lang.OutOfMemoryError is thrown.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_fd_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"70.0\",\"warning\":\"50.0\"}",
      "displayName" : "File Descriptor Monitoring Thresholds",
      "description" : "The health test thresholds of the number of file descriptors used. Specified as a percentage of file descriptor limit.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_web_metric_collection_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"10000.0\"}",
      "displayName" : "Web Metric Collection Duration",
      "description" : "The health test thresholds on the duration of the metrics request to the web server.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_memory_hard_limit",
      "required" : false,
      "default" : "-1",
      "displayName" : "Cgroup Memory Hard Limit",
      "description" : "Hard memory limit to assign to this role, enforced by the Linux kernel. When the limit is reached, the kernel will reclaim pages charged to the process. If reclaiming fails, the kernel may kill the process. Both anonymous as well as page cache pages contribute to the limit. Use a value of -1 B to specify no limit. By default processes not managed by Cloudera Manager will have no limit.",
      "relatedName" : "memory.limit_in_bytes",
      "validationState" : "OK"
    }, {
      "name" : "rm_cpu_shares",
      "required" : false,
      "default" : "1024",
      "displayName" : "Cgroup CPU Shares",
      "description" : "Number of CPU shares to assign to this role. The greater the number of shares, the larger the share of the host's CPUs that will be given to this role when the host experiences CPU contention. Must be between 2 and 262144. Defaults to 1024 for processes not managed by Cloudera Manager.",
      "relatedName" : "cpu.shares",
      "validationState" : "OK"
    }, {
      "name" : "SECONDARYNAMENODE_role_env_safety_valve",
      "required" : false,
      "displayName" : "SecondaryNameNode Environment Advanced Configuration Snippet (Safety Valve)",
      "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of this role except client configuration.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "stacks_collection_directory",
      "required" : false,
      "displayName" : "Stacks Collection Directory",
      "description" : "The directory in which stacks logs are placed. If not set, stacks are logged into a <span class='code'>stacks</span> subdirectory of the role's log directory.",
      "relatedName" : "stacks_collection_directory",
      "validationState" : "OK"
    }, {
      "name" : "heap_dump_directory_free_space_absolute_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"5.36870912E9\",\"warning\":\"1.073741824E10\"}",
      "displayName" : "Heap Dump Directory Free Space Monitoring Absolute Thresholds",
      "description" : "The health test thresholds for monitoring of free space on the filesystem that contains this role's heap dump directory.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "rm_io_weight",
      "required" : false,
      "default" : "500",
      "displayName" : "Cgroup I/O Weight",
      "description" : "Weight for the read I/O requests issued by this role. The greater the weight, the higher the priority of the requests when the host experiences I/O contention. Must be between 100 and 1000. Defaults to 1000 for processes not managed by Cloudera Manager.",
      "relatedName" : "blkio.weight",
      "validationState" : "OK"
    }, {
      "name" : "hadoop_metrics2_safety_valve",
      "required" : false,
      "displayName" : "Hadoop Metrics2 Advanced Configuration Snippet (Safety Valve)",
      "description" : "Advanced Configuration Snippet (Safety Valve) for Hadoop Metrics2. Properties will be inserted into <strong>hadoop-metrics2.properties</strong>.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "dfs_secondary_https_port",
      "required" : false,
      "default" : "50495",
      "displayName" : "Secure SecondaryNameNode Web UI Port (SSL)",
      "description" : "The base port where the secure SecondaryNameNode web UI listens.",
      "relatedName" : "dfs.secondary.https.port",
      "validationState" : "OK"
    }, {
      "name" : "enable_config_alerts",
      "required" : false,
      "default" : "false",
      "displayName" : "Enable Configuration Change Alerts",
      "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "secondarynamenode_web_metric_collection_enabled",
      "required" : false,
      "default" : "true",
      "displayName" : "Web Metric Collection",
      "description" : "Enables the health test that the Cloudera Manager Agent can successfully contact and gather metrics from the web server.",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "role_triggers",
      "required" : false,
      "default" : "[]",
      "displayName" : "Role Triggers",
      "description" : "<p>The configured triggers for this role. This is a JSON formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has all of the following fields:</p><ul><li><code>triggerName</code> <strong>(mandatory)</strong> - The name of the trigger. This value must be unique for the specific role. </li><li><code>triggerExpression</code> <strong>(mandatory)</strong> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <strong>(optional)</strong> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <strong> (optional)</strong> - By default set to 'true'. If set to 'false', the trigger will not be evaluated.</li><li><code>expressionEditorConfig</code> <strong> (optional)</strong> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here may lead to inconsistencies.</li></ul></p><p>For example, the following JSON formatted trigger configured for a DataNode fires if the DataNode has more than 1500 file-descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleName=$ROLENAME and last(fd_open) > 1500) DO health:bad\",\n  \"streamThreshold\": 0, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change in the future and, as a result, backward compatibility is not guaranteed between releases at this time.</p>",
      "relatedName" : "",
      "validationState" : "OK"
    }, {
      "name" : "process_swap_memory_thresholds",
      "required" : false,
      "default" : "{\"critical\":\"never\",\"warning\":\"any\"}",
      "displayName" : "Process Swap Memory Thresholds",
      "description" : "The health test thresholds on the swap memory usage of the process.",
      "relatedName" : "",
      "validationState" : "OK"
    } ]
  } ],
  "items" : [ {
    "name" : "dfs_block_local_path_access_user",
    "value" : "impala",
    "required" : false,
    "displayName" : "DataNode Local Path Access Users",
    "description" : "Comma separated list of users allowed to do short circuit read. A short circuit read allows a client co-located with the data to read HDFS file blocks directly from HDFS. If empty, will default to the DataNode process' user.",
    "relatedName" : "dfs.block.local-path-access.user",
    "validationState" : "OK"
  }, {
    "name" : "zookeeper_service",
    "value" : "zookeeper",
    "required" : false,
    "displayName" : "ZooKeeper Service",
    "description" : "Name of the ZooKeeper service that this HDFS service instance depends on",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "redaction_policy_enabled",
    "required" : false,
    "default" : "false",
    "displayName" : "Enable Log and Query Redaction",
    "description" : "Enable/Disable the Log and Query Redaction Policy for this cluster.",
    "relatedName" : "redaction_policy_enabled",
    "validationState" : "OK"
  }, {
    "name" : "httpfs_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "HttpFS Proxy User Groups",
    "description" : "Comma-delimited list of groups to allow the HttpFS user to impersonate. The default '*' allows all groups. To disable entirely, use a string that does not correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.httpfs.groups",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_namenode_health_enabled",
    "required" : false,
    "default" : "true",
    "displayName" : "Active NameNode Role Health Check",
    "description" : "When computing the overall HDFS cluster health, consider the active NameNode's health",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_client_use_datanode_hostname",
    "required" : false,
    "default" : "false",
    "displayName" : "Use DataNode Hostname",
    "description" : "Typically, HDFS clients and servers communicate by opening sockets via an IP address. In certain networking configurations, it is preferable to open sockets after doing a DNS lookup on the hostname. Enable this property to open sockets after doing a DNS lookup on the hostname. This property is supported in CDH3u4 or later deployments.",
    "relatedName" : "dfs.client.use.datanode.hostname",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_active_namenode_detecton_window",
    "required" : false,
    "default" : "3",
    "displayName" : "Active NameNode Detection Window",
    "description" : "The tolerance window that will be used in HDFS service tests that depend on detection of the active NameNode.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "audit_event_log_dir",
    "required" : false,
    "default" : "/var/log/hadoop-hdfs/audit",
    "displayName" : "Audit Log Directory",
    "description" : "Path to the directory where audit logs will be written. The directory will be created if it doesn't exist.",
    "relatedName" : "audit_event_log_dir",
    "validationState" : "OK"
  }, {
    "name" : "dfs_encrypt_data_transfer_algorithm",
    "required" : false,
    "default" : "rc4",
    "displayName" : "Data Transfer Encryption Algorithm",
    "description" : "Algorithm to encrypt data transfer between DataNodes and clients, and among DataNodes. 3des is more cryptographically secure, but rc4 is substantially faster.",
    "relatedName" : "dfs.encrypt.data.transfer.algorithm",
    "validationState" : "OK"
  }, {
    "name" : "smon_derived_configs_safety_valve",
    "required" : false,
    "displayName" : "Service Monitor Derived Configs Advanced Configuration Snippet (Safety Valve)",
    "description" : "For advanced use only, a list of derived configuration properties that will be used by the Service Monitor instead of the default ones.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "catch_events",
    "required" : false,
    "default" : "true",
    "displayName" : "Enable Log Event Capture",
    "description" : "When set, each role identifies important log events and forwards them to Cloudera Manager.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_user_to_impersonate",
    "required" : false,
    "displayName" : "HDFS User to Impersonate",
    "description" : "The user the management services impersonates when connecting to HDFS. If no value is specified, the HDFS superuser is used.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "mapred_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Mapred Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you want to allow the mapred user to impersonate other users. The default '*' allows all hosts. To disable entirely, use a string that doesn't correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.mapred.hosts",
    "validationState" : "OK"
  }, {
    "name" : "oozie_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Oozie Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you want to allow the oozie user to impersonate other users. The default '*' allows all hosts. To disable entirely, use a string that doesn't correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.oozie.hosts",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_group_name_attr",
    "required" : false,
    "default" : "cn",
    "displayName" : "Hadoop User Group Mapping LDAP Group Name Attribute",
    "description" : "The attribute of the group object that identifies the group name. The default will usually be appropriate for all LDAP systems.",
    "relatedName" : "hadoop.security.group.mapping.ldap.search.attr.group.name",
    "validationState" : "OK"
  }, {
    "name" : "firehose_hdfs_canary_directory",
    "required" : false,
    "default" : "/tmp/.cloudera_health_monitoring_canary_files",
    "displayName" : "HDFS Health Canary Directory",
    "description" : "The service monitor will use this directory to create files to test if the hdfs service is healthy. The directory and files are created with permissions specified by 'HDFS Health Canary Directory Permissions'",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_ha_fencing_methods",
    "required" : false,
    "default" : "shell(./cloudera_manager_agent_fencer.py)",
    "displayName" : "HDFS High Availability Fencing Methods",
    "description" : "List of fencing methods to use for service fencing. <tt>shell(./cloudera_manager_agent_fencer.py)</tt> is a fencing mechanism designed to use the Cloudera Manager agent.  The <tt>sshfence</tt> method uses SSH.  If using custom fencers (that may communicate with shared store, power units, or network switches), use the shell to invoke them.",
    "relatedName" : "dfs.ha.fencing.methods",
    "validationState" : "OK"
  }, {
    "name" : "HTTP_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "HTTP Proxy User Groups",
    "description" : "Comma-delimited list of groups that you want to allow the HTTP user to impersonate. The default '*' allows all groups. To disable entirely, use a string that doesn't correspond to a group name, such as '_no_group_'. This is used by WebHCat.",
    "relatedName" : "hadoop.proxyuser.HTTP.groups",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_group_filter",
    "required" : false,
    "default" : "(objectClass=group)",
    "displayName" : "Hadoop User Group Mapping LDAP Group Search Filter",
    "description" : "An additional filter to use when searching for groups.",
    "relatedName" : "hadoop.security.group.mapping.ldap.search.filter.group",
    "validationState" : "OK"
  }, {
    "name" : "enable_alerts",
    "required" : false,
    "default" : "true",
    "displayName" : "Enable Service Level Health Alerts",
    "description" : "When set, Cloudera Manager will send alerts when the health of this service reaches the threshold specified by the EventServer setting eventserver_health_events_alert_threshold",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "redaction_policy",
    "required" : false,
    "displayName" : "Log and Query Redaction Policy",
    "description" : "<p><b>Note:</b> Do not edit this property in the classic layout. Switch to the new layout to be able to use preconfigured redaction rules and test your rules inline.</p><p>Use this property to define a list of rules to be followed for redacting sensitive information from log files and query strings. Click + to add a new redaction rule. You can choose between one of the preconfigured rules or add a custom rule. When specifying a custom rule, the Search field should contain a regular expression that will be matched against the data. If a match is found, it will be replaced by the contents of the Replace field.</p><p>Trigger is an optional field. It can be used to specify a simple string which will be searched in the data. If the string is found, only then will the redactor attempt to find a match for the Search regex. If no Trigger is specified, redaction will always be carried out by matching the Search regular expression. The Trigger field exists solely for performance reasons: simple string matching is faster than regular expression matching.</p><p>Test your rules by entering sample text into the Test Redaction Rules text box and click Test Redaction. If no rules match, the text you entered will be returned unchanged.</p>",
    "relatedName" : "redaction_policy",
    "validationState" : "OK"
  }, {
    "name" : "dfs_permissions",
    "required" : false,
    "default" : "true",
    "displayName" : "Check HDFS Permissions",
    "description" : "If false, permission checking is turned off for files in HDFS.",
    "relatedName" : "dfs.permissions",
    "validationState" : "OK"
  }, {
    "name" : "dfs_replication_max",
    "required" : false,
    "default" : "512",
    "displayName" : "Maximal Block Replication",
    "description" : "The maximal block replication.",
    "relatedName" : "dfs.replication.max",
    "validationState" : "OK"
  }, {
    "name" : "dfs_ha_fencing_ssh_private_key_files",
    "required" : false,
    "displayName" : "Private Keys for SSH Fencing Strategy",
    "description" : "The SSH private key files to use with the built-in sshfence fencer. These are to be accessible to the <tt>hdfs</tt> user on the machines running the NameNodes.",
    "relatedName" : "dfs.ha.fencing.ssh.private-key-files",
    "validationState" : "OK"
  }, {
    "name" : "navigator_audit_enabled",
    "required" : false,
    "default" : "true",
    "displayName" : "Enable Audit Collection",
    "description" : "Enable collection of audit events from the service's roles.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "kms_service",
    "required" : false,
    "displayName" : "KMS Service",
    "description" : "The Key Management Server used by HDFS. This must be set to use encryption for data at rest.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "service_triggers",
    "required" : false,
    "default" : "[]",
    "displayName" : "Service Triggers",
    "description" : "<p>The configured triggers for this service. This is a JSON formatted list of triggers. These triggers are evaluated as part as the health system. Every trigger expression is parsed, and if the trigger condition is met, the list of actions provided in the trigger expression is executed.</p><p>Each trigger has all of the following fields:</p><ul><li><code>triggerName</code> <strong>(mandatory)</strong> - The name of the trigger. This value must be unique for the specific service. </li><li><code>triggerExpression</code> <strong>(mandatory)</strong> - A tsquery expression representing the trigger. </li><li><code>streamThreshold</code> <strong>(optional)</strong> - The maximum number of streams that can satisfy a condition of a trigger before the condition fires. By default set to 0, and any stream returned causes the condition to fire. </li><li><code>enabled</code> <strong> (optional)</strong> - By default set to 'true'. If set to 'false', the trigger will not be evaluated.</li><li><code>expressionEditorConfig</code> <strong> (optional)</strong> - Metadata for the trigger editor. If present, the trigger should only be edited from the Edit Trigger page; editing the trigger here may lead to inconsistencies.</li></ul></p><p>For example, the followig JSON formatted trigger fires if there are more than 10 DataNodes with more than 500 file-descriptors opened:</p><p><pre>[{\"triggerName\": \"sample-trigger\",\n  \"triggerExpression\": \"IF (SELECT fd_open WHERE roleType = DataNode and last(fd_open) > 500) DO health:bad\",\n  \"streamThreshold\": 10, \"enabled\": \"true\"}]</pre></p><p>See the trigger rules documentation for more details on how to write triggers using tsquery.</p><p>The JSON format is evolving and may change in the future and, as a result, backward compatibility is not guaranteed between releases at this time.</p>",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_ha_fencing_ssh_connect_timeout",
    "required" : false,
    "default" : "30000",
    "displayName" : "Timeout for SSH Fencing Strategy",
    "description" : "SSH connection timeout, in milliseconds, to use with the built-in sshfence fencer.",
    "relatedName" : "dfs.ha.fencing.ssh.connect-timeout",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_use_ssl",
    "required" : false,
    "default" : "false",
    "displayName" : "Hadoop User Group Mapping LDAP SSL Enabled",
    "description" : "Whether or not to use SSL when connecting to the LDAP server.",
    "relatedName" : "hadoop.security.group.mapping.ldap.use.ssl",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_security_authorization",
    "required" : false,
    "default" : "false",
    "displayName" : "Hadoop Secure Authorization",
    "description" : "Enable authorization",
    "relatedName" : "hadoop.security.authorization",
    "validationState" : "OK"
  }, {
    "name" : "firehose_hdfs_canary_directory_permissions",
    "required" : false,
    "default" : "-rwxrwxrwx",
    "displayName" : "HDFS Health Canary Directory Permissions",
    "description" : "The service monitor will use these permissions to create the directory and files to test if the hdfs service is healthy. Permissions are specified using the 10-character unix-symbolic format e.g. '-rwxr-xr-x'.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_missing_blocks_thresholds",
    "required" : false,
    "default" : "{\"critical\":\"any\",\"warning\":\"never\"}",
    "displayName" : "Missing Block Monitoring Thresholds",
    "description" : "The health check thresholds of the number of missing blocks. Specified as a percentage of the total number of blocks.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "mapred_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Mapred Proxy User Groups",
    "description" : "Comma-delimited list of groups that you want to allow the mapred user to impersonate. The default '*' allows all groups. To disable entirely, use a string that doesn't correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.mapred.groups",
    "validationState" : "OK"
  }, {
    "name" : "navigator_client_max_num_audit_log",
    "required" : false,
    "default" : "10",
    "displayName" : "Number of Audit Logs to Retain",
    "description" : "Maximum number of rolled over audit logs to retain. The logs will not be deleted if they contain audit events that have not yet been propagated to Audit Server.",
    "relatedName" : "navigator.client.max_num_audit_log",
    "validationState" : "OK"
  }, {
    "name" : "smon_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Service Monitor Proxy User Groups",
    "description" : "Allows the Cloudera Service Monitor user to impersonate any members of a comma-delimited list of groups. The default '*' allows all groups. This property is used only if Service Monitor is using a different Kerberos principal than the Hue service. To disable entirely, use a string that does not correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.smon.groups",
    "validationState" : "OK"
  }, {
    "name" : "dfs_replication",
    "required" : false,
    "default" : "3",
    "displayName" : "Replication Factor",
    "description" : "Default block replication. The number of replications to make when the file is created. The default value is used if a replication number is not specified.",
    "relatedName" : "dfs.replication",
    "validationState" : "OK"
  }, {
    "name" : "dfs_namenode_acls_enabled",
    "required" : false,
    "default" : "false",
    "displayName" : "Enable Access Control Lists",
    "description" : "ACLs (Access Control Lists) enhance the existing HDFS permission model to support controlling file access for arbitrary combinations of users and groups instead of a single owner, single group, and all other users. When ACLs are disabled, the NameNode rejects all attempts to set an ACL.",
    "relatedName" : "dfs.namenode.acls.enabled",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_sentry_sync_enable",
    "required" : false,
    "default" : "false",
    "displayName" : "Enable Sentry Synchronization",
    "description" : "Enable automatic synchronization of HDFS ACLs with Sentry privileges. HDFS Access Control Lists and Check HDFS Permissions must be enabled when this feature is enabled. Use Sentry Synchronization Path Prefixes to define the HDFS regions where authorization is enforced using Sentry information. For more information, see <a class=\"bold\" href=\"http://tiny.cloudera.com/sentry-sync-cm5\" target=\"_blank\">Synchronizing HDFS ACLs and Sentry Authorization<i class=\"externalLink\"></i></a>.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "trusted_realms",
    "required" : false,
    "default" : "",
    "displayName" : "Trusted Kerberos Realms",
    "description" : "List of Kerberos realms that Hadoop services should trust. If empty, defaults to the default_realm property configured in the krb5.conf file. After changing this value and restarting the service, all services depending on this service must also be restarted. Adds mapping rules for each domain to the hadoop.security.auth_to_local property in core-site.xml.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_datanode_hdfs_blocks_metadata_enabled",
    "required" : false,
    "default" : "true",
    "displayName" : "Enable HDFS Block Metadata API",
    "description" : "Enables DataNode support for the experimental DistributedFileSystem.getFileVBlockStorageLocations API. Applicable to CDH 4.1 and onwards.",
    "relatedName" : "dfs.datanode.hdfs-blocks-metadata.enabled",
    "validationState" : "OK"
  }, {
    "name" : "dfs_replication_min",
    "required" : false,
    "default" : "1",
    "displayName" : "Minimal Block Replication",
    "description" : "The minimal block replication.",
    "relatedName" : "dfs.namenode.replication.min",
    "validationState" : "OK"
  }, {
    "name" : "navigator_client_config_safety_valve",
    "required" : false,
    "displayName" : "HDFS Client Advanced Configuration Snippet (Safety Valve) for navigator.client.properties",
    "description" : "For advanced use only, a string to be inserted into the client configuration for <strong>navigator.client.properties</strong>.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_domain_socket_path",
    "required" : false,
    "default" : "/var/run/hdfs-sockets/dn",
    "displayName" : "UNIX Domain Socket path",
    "description" : "Path on the DataNode's local file system to a UNIX domain socket used for communication between the DataNode and local HDFS clients. This socket is used for Short Circuit Reads. Only the HDFS System User and \"root\" should have write access to the parent directory and all of its ancestors. This property is supported in CDH 4.2 or later deployments.",
    "relatedName" : "dfs.domain.socket.path",
    "validationState" : "OK"
  }, {
    "name" : "process_groupname",
    "required" : false,
    "default" : "hdfs",
    "displayName" : "System Group",
    "description" : "The group that this service's processes should run as (except the HttpFS server, which has its own group)",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_block_size",
    "required" : false,
    "default" : "134217728",
    "displayName" : "HDFS Block Size",
    "description" : "The default block size in bytes for new HDFS files. Note that this value is also used as the HBase Region Server HLog block size.",
    "relatedName" : "dfs.blocksize",
    "validationState" : "OK"
  }, {
    "name" : "smon_client_config_overrides",
    "required" : false,
    "default" : "<property><name>dfs.socket.timeout</name><value>3000</value></property><property><name>dfs.datanode.socket.write.timeout</name><value>3000</value></property><property><name>ipc.client.connect.max.retries</name><value>1</value></property><property><name>fs.permissions.umask-mode</name><value>000</value></property>",
    "displayName" : "Service Monitor Client Config Overrides",
    "description" : "For advanced use only, a list of configuration properties that will be used by the Service Monitor instead of the current client configuration for the service.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "navigator_audit_queue_policy",
    "required" : false,
    "default" : "DROP",
    "displayName" : "Audit Queue Policy",
    "description" : "Action to take when the audit event queue is full. Drop the event or shutdown the affected process.",
    "relatedName" : "navigator.batch.queue_policy",
    "validationState" : "OK"
  }, {
    "name" : "sentry_authorization_provider_hdfs_group",
    "required" : false,
    "default" : "hive",
    "displayName" : "Sentry Authorization Provider Group",
    "description" : "For paths where authorization is enforced by Sentry Synchronization, file permissions will use this parameter as the group. This group should normally include the hive and impala users.",
    "relatedName" : "sentry.authorization-provider.hdfs-group",
    "validationState" : "OK"
  }, {
    "name" : "dfs_umaskmode",
    "required" : false,
    "default" : "022",
    "displayName" : "Default Umask",
    "description" : "Default umask for file and directory creation, specified in an octal value (with a leading 0)",
    "relatedName" : "fs.permissions.umask-mode",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_rpc_protection",
    "required" : false,
    "default" : "authentication",
    "displayName" : "Hadoop RPC Protection",
    "description" : "Quality of protection for secured RPC connections between NameNode and HDFS clients. For effective RPC protection, enable Kerberos authentication.",
    "relatedName" : "hadoop.rpc.protection",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_standby_namenodes_health_enabled",
    "required" : false,
    "default" : "true",
    "displayName" : "Standby NameNode Health Check",
    "description" : "When computing the overall HDFS cluster health, consider the health of the standby NameNode.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "flume_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Flume Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you want to allow the flume user to impersonate other users. The default '*' allows all hosts. To disable entirely, use a string that doesn't correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.flume.hosts",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_hadoop_group_name",
    "required" : false,
    "default" : "hadoop",
    "displayName" : "Shared Hadoop Group Name",
    "description" : "The name of the system group shared by all the core Hadoop services.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_security_authentication",
    "required" : false,
    "default" : "simple",
    "displayName" : "Hadoop Secure Authentication",
    "description" : "Choose the authentication mechanism used by Hadoop",
    "relatedName" : "hadoop.security.authentication",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_namenode_activation_startup_tolerance",
    "required" : false,
    "default" : "180",
    "displayName" : "NameNode Activation Startup Tolerance",
    "description" : "The amount of time after NameNode(s) start that the lack of an active NameNode will be tolerated. This is intended to allow either the auto-failover daemon to make a NameNode active, or a specifically issued failover command to take effect. This is an advanced option that does not often need to be changed.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_url",
    "required" : false,
    "default" : "",
    "displayName" : "Hadoop User Group Mapping LDAP URL",
    "description" : "The URL for the LDAP server to use for resolving user groups when using LdapGroupsMapping.",
    "relatedName" : "hadoop.security.group.mapping.ldap.url",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_datanodes_healthy_thresholds",
    "required" : false,
    "default" : "{\"critical\":\"90.0\",\"warning\":\"95.0\"}",
    "displayName" : "Healthy DataNode Monitoring Thresholds",
    "description" : "The health test thresholds of the overall DataNode health. The check returns \"Concerning\" health if the percentage of \"Healthy\" DataNodes falls below the warning threshold. The check is unhealthy if the total percentage of \"Healthy\" and \"Concerning\" DataNodes falls below the critical threshold.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_service_config_safety_valve",
    "required" : false,
    "displayName" : "HDFS Service Advanced Configuration Snippet (Safety Valve) for hdfs-site.xml",
    "description" : "For advanced use only, a string to be inserted into <strong>hdfs-site.xml</strong>. Applies to configurations of all roles in this service except client configuration.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_hadoop_ssl_enabled",
    "required" : false,
    "default" : "false",
    "displayName" : "Hadoop SSL Enabled",
    "description" : "Enable SSL encryption for HDFS, MapReduce, and YARN web UIs, as well as encrypted shuffle for MapReduce and Yarn.",
    "relatedName" : "hadoop.ssl.enabled",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_base",
    "required" : false,
    "default" : "",
    "displayName" : "Hadoop User Group Mapping Search Base",
    "description" : "The search base for the LDAP connection. This is a distinguished name, and will typically be the root of the LDAP directory.",
    "relatedName" : "hadoop.security.group.mapping.ldap.base",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_ssl_server_safety_valve",
    "required" : false,
    "displayName" : "HDFS Service Advanced Configuration Snippet (Safety Valve) for ssl-server.xml",
    "description" : "For advanced use only, a string to be inserted into <strong>ssl-server.xml</strong>. Applies to configurations of all roles in this service except client configuration.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_image_transfer_bandwidthPerSec",
    "required" : false,
    "default" : "0",
    "displayName" : "FsImage Transfer Bandwidth",
    "description" : "Maximum bandwidth used for image transfer in bytes per second. This can help keep normal NameNode operations responsive during checkpointing. A default value of 0 indicates that throttling is disabled.",
    "relatedName" : "dfs.image.transfer.bandwidthPerSec",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_replication_env_safety_valve",
    "required" : false,
    "displayName" : "HDFS Replication Advanced Configuration Snippet (Safety Valve)",
    "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into the environment of HDFS replication jobs.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_user_home_dir",
    "required" : false,
    "default" : "/var/lib/hadoop-hdfs",
    "displayName" : "System User's Home Directory",
    "description" : "The home directory of the system user on the local filesystem. This setting must reflect the system's configured value - only changing it here will not change the actual home directory.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "ssl_client_truststore_password",
    "required" : false,
    "displayName" : "Cluster-Wide Default SSL Client Truststore Password",
    "description" : "Password for the SSL client truststore. Defines a cluster-wide default that can be overridden by individual services.",
    "relatedName" : "ssl.client.truststore.password",
    "validationState" : "OK"
  }, {
    "name" : "dfs_client_file_block_storage_locations_timeout",
    "required" : false,
    "default" : "10000",
    "displayName" : "HDFS File Block Storage Location Timeout",
    "description" : "Timeout in milliseconds for the parallel RPCs made in DistributedFileSystem#getFileBlockStorageLocations(). This value is only emitted for Impala.",
    "relatedName" : "dfs.client.file-block-storage-locations.timeout.millis",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "HDFS Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you want to allow the HDFS user to impersonate other users. The default '*' allows all hosts. To disable entirely, use a string that doesn't correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.hdfs.hosts",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_policy_config_safety_valve",
    "required" : false,
    "displayName" : "HDFS Service Advanced Configuration Snippet (Safety Valve) for hadoop-policy.xml",
    "description" : "For advanced use only, a string to be inserted into <strong>hadoop-policy.xml</strong>. Applies to configurations of all roles in this service except client configuration.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_keystore",
    "required" : false,
    "default" : "",
    "displayName" : "Hadoop User Group Mapping LDAP SSL Keystore",
    "description" : "File path to the SSL keystore containing the SSL certificate required by the LDAP server.",
    "relatedName" : "hadoop.security.group.mapping.ldap.ssl.keystore",
    "validationState" : "OK"
  }, {
    "name" : "dfs_ha_fencing_cloudera_manager_timeout_millis",
    "required" : false,
    "default" : "10000",
    "displayName" : "Timeout for Cloudera Manager Fencing Strategy",
    "description" : "The timeout, in milliseconds, to use with the Cloudera Manager agent-based fencer.",
    "relatedName" : "dfs.ha.fencing.cloudera_manager.timeout_millis",
    "validationState" : "OK"
  }, {
    "name" : "navigator_event_tracker",
    "required" : false,
    "default" : "{\n  \"comment\" : [\n    \"The default event tracker for HDFS services defines equality by \",\n    \"comparing the username, operation, and source path of the events.\"\n  ],\n  \"timeToLive\" : 60000,\n  \"fields\" : [\n    { \"type\": \"value\", \"name\" : \"src\" },\n    { \"type\": \"value\", \"name\" : \"operation\" },\n    { \"type\": \"username\", \"name\" : \"username\" }\n  ]\n}\n",
    "displayName" : "Audit Event Tracker",
    "description" : "<p>\nConfigures the rules for event tracking and coalescing. This feature is\nused to define equivalency between different audit events. When\nevents match, according to a set of configurable parameters, only one\nentry in the audit list is generated for all the matching events.\n</p>\n\n<p>\nTracking works by keeping a reference to events when they first appear,\nand comparing other incoming events against the \"tracked\" events according\nto the rules defined here.\n</p>\n\n<p>Event trackers are defined in a JSON object like the following:</p>\n\n<pre>\n{\n  \"timeToLive\" : [integer],\n  \"fields\" : [\n    {\n      \"type\" : [string],\n      \"name\" : [string]\n    }\n  ]\n}\n</pre>\n\n<p>\nWhere:\n</p>\n\n<ul>\n  <li>timeToLive: maximum amount of time an event will be tracked, in\n  milliseconds. Must be provided. This defines how long, since it's\n  first seen, an event will be tracked. A value of 0 disables tracking.</li>\n\n  <li>fields: list of fields to compare when matching events against\n  tracked events.</li>\n</ul>\n\n<p>\nEach field has an evaluator type associated with it. The evaluator defines\nhow the field data is to be compared. The following evaluators are\navailable:\n</p>\n\n<ul>\n  <li>value: uses the field value for comparison.</li>\n\n  <li>username: treats the field value as a user name, and ignores any\n  host-specific data. This is useful for environment using Kerberos,\n  so that only the principal name and realm are compared.</li>\n</ul>\n\n<p>\nThe following is the list of fields that can be used to compare HDFS events:\n</p>\n\n<ul>\n  <li>operation: the HDFS operation being performed.</li>\n  <li>username: the user performing the action.</li>\n  <li>ipAddress: the IP from where the request originated.</li>\n  <li>allowed: whether the operation was allowed or denied.</li>\n  <li>src: the source path for the operation.</li>\n  <li>dest: the destination path for the operation.</li>\n  <li>permissions: the permissions associated with the operation.</li>\n</ul>\n\n<p>\nThe default event tracker for HDFS services defines equality by comparing the\nusername, operation, and source path of the events.\n</p>\n",
    "relatedName" : "navigator_event_tracker",
    "validationState" : "OK"
  }, {
    "name" : "failover_controllers_healthy_enabled",
    "required" : false,
    "default" : "true",
    "displayName" : "Failover Controllers Healthy",
    "description" : "Enables the health check that verifies that the failover controllers associated with this service are healthy and running.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_authorized_admin_users",
    "required" : false,
    "default" : "*",
    "displayName" : "Authorized Admin Users",
    "description" : "Comma-separated list of users authorized to perform admin operations on Hadoop. This is emitted only if authorization is enabled.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_data_transfer_protection",
    "required" : false,
    "displayName" : "DataNode Data Transfer Protection",
    "description" : "SASL protection mode for secured connections to the DataNodes when reading or writing data.",
    "relatedName" : "dfs.data.transfer.protection",
    "validationState" : "OK"
  }, {
    "name" : "httpfs_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "HttpFS Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you allow the HttpFS user to impersonate other users. The default '*' allows all hosts. To disable entirely, use a string that doesn't correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.httpfs.hosts",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_secure_web_ui",
    "required" : false,
    "default" : "false",
    "displayName" : "Enable Kerberos Authentication for HTTP Web-Consoles",
    "description" : "Enables Kerberos authentication for Hadoop HTTP web consoles for all roles of this service using the SPNEGO protocol. <b>Note:</b> This is effective only if Kerberos is enabled for the HDFS service.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_authorized_users",
    "required" : false,
    "default" : "*",
    "displayName" : "Authorized Users",
    "description" : "Comma-separated list of users authorized to used Hadoop. This is emitted only if authorization is enabled.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "HDFS Proxy User Groups",
    "description" : "Comma-delimited list of groups to allow the HDFS user to impersonate. The default '*' allows all groups. To disable entirely, use a string that does not correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.hdfs.groups",
    "validationState" : "OK"
  }, {
    "name" : "hive_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Hive Proxy User Groups",
    "description" : "Comma-delimited list of groups that you want to allow the Hive user to impersonate. The default '*' allows all groups. To disable entirely, use a string that doesn't correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.hive.groups",
    "validationState" : "OK"
  }, {
    "name" : "core_site_safety_valve",
    "required" : false,
    "displayName" : "Cluster-wide Advanced Configuration Snippet (Safety Valve) for core-site.xml",
    "description" : "For advanced use only, a string to be inserted into <strong>core-site.xml</strong>. Applies to all roles and client configurations in this HDFS service as well as all its dependent services. Any configs added here will be overridden by their default values in HDFS (which can be found in hdfs-default.xml).",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "smon_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Service Monitor Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you want to allow the Cloudera Service Monitor user to impersonate other users. The default '*' allows all hosts. This property is used only if Service Monitor is using a different Kerberos principal than the Hue service. To disable entirely, use a string that does not correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.smon.hosts",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_service_env_safety_valve",
    "required" : false,
    "displayName" : "HDFS Service Environment Advanced Configuration Snippet (Safety Valve)",
    "description" : "For advanced use only, key-value pairs (one on each line) to be inserted into a role's environment. Applies to configurations of all roles in this service except client configuration.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hue_kerberos_principal_shortname",
    "required" : false,
    "displayName" : "Hue's Kerberos Principal Short Name",
    "description" : "The short name of the Hue Kerberos principal. Normally, you do not need to specify this configuration. Cloudera Manager auto-configures this property so that Hue and Cloudera Manamgent Service work properly.",
    "relatedName" : "hue.kerberos.principal.shortname",
    "validationState" : "OK"
  }, {
    "name" : "dfs_permissions_supergroup",
    "required" : false,
    "default" : "supergroup",
    "displayName" : "Superuser Group",
    "description" : "The name of the group of superusers.",
    "relatedName" : "dfs.permissions.superusergroup",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_keystore_passwd",
    "required" : false,
    "default" : "",
    "displayName" : "Hadoop User Group Mapping LDAP SSL Keystore Password",
    "description" : "The password for the SSL keystore.",
    "relatedName" : "hadoop.security.group.mapping.ldap.ssl.keystore.password",
    "validationState" : "OK"
  }, {
    "name" : "dfs_encrypt_data_transfer",
    "required" : false,
    "default" : "false",
    "displayName" : "Enable Data Transfer Encryption",
    "description" : "Enable encryption of data transfer between DataNodes and clients, and among DataNodes. For effective data transfer protection, enable Kerberos authentication and choose Privacy Quality of RPC Protection.",
    "relatedName" : "dfs.encrypt.data.transfer",
    "validationState" : "OK"
  }, {
    "name" : "navigator_audit_event_filter",
    "required" : false,
    "default" : "{\n  \"comment\" : [\n    \"The default audit event filter discards events generated by the internal \",\n    \"Cloudera and Hadoop users (cloudera-scm, dr.who, hbase, hive, impala, \",\n    \"mapred, solr, and spark), 'ls' actions performed by the hdfs user, \",\n    \"operations in the Cloudera Hive canary directory, and events that affect \",\n    \"files in the /tmp directory.\"\n  ],\n  \"defaultAction\" : \"accept\",\n  \"rules\" : [\n    {\n      \"action\" : \"discard\",\n      \"fields\" : [\n        {\n          \"name\" : \"username\",\n          \"match\" : \"(?:cloudera-scm|dr.who|hbase|hive|impala|mapred|solr|spark)(?:/.+)?\"\n        }\n      ]\n    },\n    {\n      \"action\" : \"discard\",\n      \"fields\" : [\n        {\n          \"name\" : \"username\",\n          \"match\" : \"(?:hdfs)(?:/.+)?\"\n        },\n        {\n          \"name\" : \"operation\",\n          \"match\" : \"(?:listStatus|listCachePools|listCacheDirectives|getfileinfo)\"\n        }\n      ]\n    },\n    {\n      \"action\" : \"discard\",\n      \"fields\" : [\n        { \"name\" : \"src\", \"match\" : \"/user/hue/\\\\.cloudera_manager_hive_metastore_canary(?:/.*)?\" }\n      ]\n    },\n    {\n      \"action\" : \"discard\",\n      \"fields\" : [\n        { \"name\" : \"src\", \"match\" : \"/user/hue/\\\\.Trash/Current/user/hue/\\\\.cloudera_manager_hive_metastore_canary(?:/.*)?\" }\n      ]\n    },\n    {\n      \"action\" : \"discard\",\n      \"fields\" : [\n        { \"name\" : \"src\", \"match\" : \"/tmp(?:/.*)?\" }\n      ]\n    }\n  ]\n}\n",
    "displayName" : "Audit Event Filter",
    "description" : "<p>\nEvent filters are defined in a JSON object like the following:\n</p>\n\n<pre>\n{\n  \"defaultAction\" : (\"accept\", \"discard\"),\n  \"rules\" : [\n    {\n      \"action\" : (\"accept\", \"discard\"),\n      \"fields\" : [\n        {\n          \"name\" : \"fieldName\",\n          \"match\" : \"regex\"\n        }\n      ]\n    }\n  ]\n}\n</pre>\n\n<p>\nA filter has a default action and a list of rules, in order of precedence.\nEach rule defines an action, and a list of fields to match against the\naudit event.\n</p>\n\n<p>\nA rule is \"accepted\" if all the listed field entries match the audit\nevent. At that point, the action declared by the rule is taken.\n</p>\n\n<p>\nIf no rules match the event, the default action is taken. Actions\ndefault to \"accept\" if not defined in the JSON object.\n</p>\n\n<p>\nThe following is the list of fields that can be filtered for HDFS events:\n</p>\n\n<ul>\n  <li>username: the user performing the action.</li>\n  <li>ipAddress: the IP from where the request originated.</li>\n  <li>command: the HDFS operation being performed.</li>\n  <li>src: the source path for the operation.</li>\n  <li>dest: the destination path for the operation.</li>\n  <li>permissions: the permissions associated with the operation.</li>\n</ul>\n\n<p>\nThe default audit event filter discards events generated by the internal\nCloudera and Hadoop users (cloudera-scm, dr.who, hbase, hive, impala, mapred,\nsolr, and spark), 'ls' actions performed by the hdfs user, operations in the\nCloudera Hive canary directory, and events that affect files in the /tmp\ndirectory.\"\n</p>\n",
    "relatedName" : "navigator.event.filter",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_free_space_thresholds",
    "required" : false,
    "default" : "{\"critical\":\"10.0\",\"warning\":\"20.0\"}",
    "displayName" : "HDFS Free Space Monitoring Thresholds",
    "description" : "The health check thresholds of free space in HDFS. Specified as a percentage of total HDFS capacity.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "yarn_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "YARN Proxy User Groups",
    "description" : "Comma-delimited list of groups that you want to allow the YARN user to impersonate. The default '*' allows all groups. To disable entirely, use a string that does not correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.yarn.groups",
    "validationState" : "OK"
  }, {
    "name" : "kerberos_princ_name",
    "required" : false,
    "default" : "hdfs",
    "displayName" : "Kerberos Principal",
    "description" : "Kerberos principal short name used by all roles of this service.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_ha_proxy_provider",
    "required" : false,
    "default" : "org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider",
    "displayName" : "FailoverProxyProvider Class",
    "description" : "Enter a FailoverProxyProvider implementation to configure two URIs to connect to during fail-over. The first configured address is tried first, and on a fail-over event the other address is tried.",
    "relatedName" : "dfs.client.failover.proxy.provider",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_http_auth_cookie_domain",
    "required" : false,
    "default" : "",
    "displayName" : "Hadoop HTTP Authentication Cookie Domain",
    "description" : "The domain to use for the HTTP cookie that stores the authentication token. In order for authentiation to work correctly across all Hadoop nodes' web-consoles the domain must be correctly set. <b>Important:</b> when using IP addresses, browsers ignore cookies with domain settings. For this setting to work properly all nodes in the cluster must be configured to generate URLs with hostname.domain names on it.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_ssl_client_safety_valve",
    "required" : false,
    "displayName" : "HDFS Advanced Configuration Snippet (Safety Valve) for ssl-client.xml",
    "description" : "For advanced use only, a string to be inserted into <strong>ssl-client.xml</strong>. Applies cluster-wide, but can be overridden by individual services.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_datanode_read_shortcircuit",
    "required" : false,
    "default" : "true",
    "displayName" : "Enable HDFS Short Circuit Read",
    "description" : "Enable HDFS short circuit read. This allows a client co-located with the DataNode to read HDFS file blocks directly. This gives a performance boost to distributed clients that are aware of locality.",
    "relatedName" : "dfs.client.read.shortcircuit",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_canary_health_enabled",
    "required" : false,
    "default" : "true",
    "displayName" : "HDFS Canary Health Check",
    "description" : "Enables the health check that a client can create, read, write, and delete files",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "extra_auth_to_local_rules",
    "required" : false,
    "displayName" : "Additional Rules to Map Kerberos Principals to Short Names",
    "description" : "Additional mapping rules that will be inserted before rules generated from the list of trusted realms and before the default rule. After changing this value and restarting the service, any services depending on this one must be restarted as well. The hadoop.security.auth_to_local property is configured using this information.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "navigator_audit_log_max_file_size",
    "required" : false,
    "default" : "100",
    "displayName" : "Maximum Audit Log File Size",
    "description" : "Maximum size of audit log file in MB before it is rolled over.",
    "relatedName" : "navigator.audit_log_max_file_size",
    "validationState" : "OK"
  }, {
    "name" : "hue_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Hue Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you want to allow the Hue user to impersonate other users. The default '*' allows all hosts. To disable entirely, use a string that doesn't correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.hue.hosts",
    "validationState" : "OK"
  }, {
    "name" : "HTTP_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "HTTP Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you want to allow the HTTP user to impersonate other users. The default '*' allows all hosts. To disable entirely, use a string that doesn't correspond to a host name, such as '_no_host'. This is used by WebHCat.",
    "relatedName" : "hadoop.proxyuser.HTTP.hosts",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_under_replicated_blocks_thresholds",
    "required" : false,
    "default" : "{\"critical\":\"40.0\",\"warning\":\"10.0\"}",
    "displayName" : "Under-replicated Block Monitoring Thresholds",
    "description" : "The health check thresholds of the number of under-replicated blocks. Specified as a percentage of the total number of blocks.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hive_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Hive Proxy User Hosts",
    "description" : "Comma-delimited list of hosts where you want to allow the Hive user to impersonate other users. The default '*' allows all hosts. To disable entirely, use a string that doesn't correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.hive.hosts",
    "validationState" : "OK"
  }, {
    "name" : "flume_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Flume Proxy User Groups",
    "description" : "Allows the flume user to impersonate any members of a comma-delimited list of groups. The default '*' allows all groups. To disable entirely, use a string that doesn't correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.flume.groups",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_blocks_with_corrupt_replicas_thresholds",
    "required" : false,
    "default" : "{\"critical\":\"1.0\",\"warning\":\"0.5\"}",
    "displayName" : "Blocks With Corrupt Replicas Monitoring Thresholds",
    "description" : "The health check thresholds of the number of blocks that have at least one corrupt replica. Specified as a percentage of the total number of blocks.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_security_group_mapping",
    "required" : false,
    "default" : "org.apache.hadoop.security.ShellBasedUnixGroupsMapping",
    "displayName" : "Hadoop User Group Mapping Implementation",
    "description" : "Class for user to group mapping (get groups for a given user).",
    "relatedName" : "hadoop.security.group.mapping",
    "validationState" : "OK"
  }, {
    "name" : "ssl_server_keystore_keypassword",
    "required" : false,
    "displayName" : "Hadoop SSL Server Keystore Key Password",
    "description" : "Password that protects the private key contained in the server keystore used for encrypted shuffle and encrypted web UIs. Applies to all configurations of daemon roles of this service.",
    "relatedName" : "ssl.server.keystore.keypassword",
    "validationState" : "OK"
  }, {
    "name" : "yarn_proxy_user_hosts_list",
    "required" : false,
    "default" : "*",
    "displayName" : "YARN Proxy User Hosts",
    "description" : "Comma-delimited list of hosts that you want to allow the YARN user to impersonate. The default '*' allows all hosts. To disable entirely, use a string that does not correspond to a host name, such as '_no_host'.",
    "relatedName" : "hadoop.proxyuser.yarn.hosts",
    "validationState" : "OK"
  }, {
    "name" : "oozie_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Oozie Proxy User Groups",
    "description" : "Allows the oozie superuser to impersonate any members of a comma-delimited list of groups. The default '*' allows all groups. To disable entirely, use a string that doesn't correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.oozie.groups",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_member_attr",
    "required" : false,
    "default" : "member",
    "displayName" : "Hadoop User Group Mapping LDAP Group Membership Attribute",
    "description" : "The attribute of the group object that identifies the users that are members of the group. The default will usually be appropriate for any LDAP installation.",
    "relatedName" : "hadoop.security.group.mapping.ldap.search.attr.member",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_authorized_groups",
    "required" : false,
    "default" : "",
    "displayName" : "Authorized Groups",
    "description" : "Comma-separated list of groups authorized to used Hadoop. This is emitted only if authorization is enabled.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "ssl_server_keystore_location",
    "required" : false,
    "displayName" : "Hadoop SSL Server Keystore File Location",
    "description" : "Path to the keystore file containing the server certificate and private key used for encrypted shuffle and encrypted web UIs. Applies to configurations of all daemon roles of this service.",
    "relatedName" : "ssl.server.keystore.location",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_bind_user",
    "required" : false,
    "default" : "",
    "displayName" : "Hadoop User Group Mapping LDAP Bind User",
    "description" : "The distinguished name of the user to bind as when connecting to the LDAP server. This may be left blank if the LDAP server supports anonymous binds.",
    "relatedName" : "hadoop.security.group.mapping.ldap.bind.user",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_user_filter",
    "required" : false,
    "default" : "(&(objectClass=user)(sAMAccountName={0}))",
    "displayName" : "Hadoop User Group Mapping LDAP User Search Filter",
    "description" : "An additional filter to use when searching for LDAP users. The default will usually be appropriate for Active Directory installations. If connecting to a generic LDAP server, ''sAMAccountName'' will likely be replaced with ''uid''. {0} is a special string used to denote where the username fits into the filter.",
    "relatedName" : "hadoop.security.group.mapping.ldap.search.filter.user",
    "validationState" : "OK"
  }, {
    "name" : "ssl_client_truststore_location",
    "required" : false,
    "displayName" : "Cluster-Wide Default SSL Client Truststore Location",
    "description" : "Path to the SSL client truststore file. Defines a cluster-wide default that can be overridden by individual services. This truststore must be in JKS format. The truststore contains certificates of trusted servers, or of Certificate Authorities trusted to identify servers. The contents of the truststore can be modified without restarting any roles. By default, changes to its contents are picked up within ten seconds. If not set, the default Java truststore is used to verify certificates.",
    "relatedName" : "ssl.client.truststore.location",
    "validationState" : "OK"
  }, {
    "name" : "process_username",
    "required" : false,
    "default" : "hdfs",
    "displayName" : "System User",
    "description" : "The user that this service's processes should run as (except the HttpFS server, which has its own user)",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hue_proxy_user_groups_list",
    "required" : false,
    "default" : "*",
    "displayName" : "Hue Proxy User Groups",
    "description" : "Comma-delimited list of groups that you want to allow the Hue user to impersonate. The default '*' allows all groups. To disable entirely, use a string that doesn't correspond to a group name, such as '_no_group_'.",
    "relatedName" : "hadoop.proxyuser.hue.groups",
    "validationState" : "OK"
  }, {
    "name" : "io_compression_codecs",
    "required" : false,
    "default" : "org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.DeflateCodec,org.apache.hadoop.io.compress.SnappyCodec,org.apache.hadoop.io.compress.Lz4Codec",
    "displayName" : "Compression Codecs",
    "description" : "Comma-separated list of compression codecs that can be used in job or map compression.",
    "relatedName" : "io.compression.codecs",
    "validationState" : "OK"
  }, {
    "name" : "enable_config_alerts",
    "required" : false,
    "default" : "false",
    "displayName" : "Enable Configuration Change Alerts",
    "description" : "When set, Cloudera Manager will send alerts when this entity's configuration changes.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "dfs_webhdfs_enabled",
    "required" : false,
    "default" : "true",
    "displayName" : "Enable WebHDFS",
    "description" : "Enable WebHDFS interface",
    "relatedName" : "dfs.webhdfs.enabled",
    "validationState" : "OK"
  }, {
    "name" : "hdfs_sentry_sync_path_prefixes",
    "required" : false,
    "default" : "/user/hive/warehouse",
    "displayName" : "Sentry Synchronization Path Prefixes",
    "description" : "A list of path prefixes that define the HDFS regions where authorization is enforced using Sentry information. Only relevant when Sentry Synchronization is enabled.",
    "relatedName" : "sentry.hdfs.integration.path.prefixes",
    "validationState" : "OK"
  }, {
    "name" : "ssl_server_keystore_password",
    "required" : false,
    "displayName" : "Hadoop SSL Server Keystore File Password",
    "description" : "Password for the server keystore file used for encrypted shuffle and encrypted web UIs. Applies to configurations of all daemon roles of this service.",
    "relatedName" : "ssl.server.keystore.password",
    "validationState" : "OK"
  }, {
    "name" : "dfs_image_transfer_timeout",
    "required" : false,
    "default" : "60000",
    "displayName" : "FsImage Transfer Timeout",
    "description" : "The amount of time to wait for HDFS filesystem image transfer from NameNode to complete.",
    "relatedName" : "dfs.image.transfer.timeout",
    "validationState" : "OK"
  }, {
    "name" : "log_event_retry_frequency",
    "required" : false,
    "default" : "30",
    "displayName" : "Log Event Retry Frequency",
    "description" : "The frequency in which the log4j event publication appender will retry sending undelivered log events to the Event server, in seconds",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_authorized_admin_groups",
    "required" : false,
    "default" : "",
    "displayName" : "Authorized Admin Groups",
    "description" : "Comma-separated list of groups authorized to perform admin operations on Hadoop. This is emitted only if authorization is enabled.",
    "relatedName" : "",
    "validationState" : "OK"
  }, {
    "name" : "hadoop_group_mapping_ldap_bind_passwd",
    "required" : false,
    "default" : "",
    "displayName" : "Hadoop User Group Mapping LDAP Bind User Password",
    "description" : "The password of the bind user.",
    "relatedName" : "hadoop.security.group.mapping.ldap.bind.password",
    "validationState" : "OK"
  } ]
}